{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bbc107",
   "metadata": {},
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f878979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alexg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Text extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import contractions\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set pd options to display all columns and rows\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text without truncation\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4aad2",
   "metadata": {},
   "source": [
    "## 1 - EDA (missing, just copy the other notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c04502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (where the notebook is)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct full paths to the CSV files\n",
    "train_path = os.path.join(BASE_DIR, \"data\", \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"data\", \"test.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e37af",
   "metadata": {},
   "source": [
    "## 2 - Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd037a",
   "metadata": {},
   "source": [
    "# missing oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9c858",
   "metadata": {},
   "source": [
    "The effectiveness of pre-processing techniques is model-dependent. For classical machine learning approaches, the optimal combination from **Symeonidis, Effrosynidis, and Arampatzis (2018). A comparative evaluation of pre-processing techniques and their interactions for twitter sentiment analysis** includes:\n",
    "\n",
    "- **URL/User Mention Replacement**: Replace URLs and user mentions with tags, as they do not contain sentiment information (Agarwal et al., 2011).\n",
    "- **Contraction Handling**: Replacing contractions improves accuracy, as contractions are common in tweets and often exempt from sentiment lexicons (Chalil et al., 2015).\n",
    "- **Number Removal**: While many researchers remove numbers (He, Lin, & Alani, 2011; Zhao, 2015), some argue that keeping numbers may improve classification effectiveness (Lin & He, 2009).\n",
    "- **Replace Punctuation Repetition**: Normalizes language and generalizes vocabulary to represent sentiment (Balahur, 2013).\n",
    "- **Lemmatization**: Passes baseline results for both datasets, especially for classic algorithms. However, it may ignore semantic information in large datasets (Shotaroo, Takamura, & Okumura, 2005)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2944197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Source: https://www.nltk.org/api/nltk.tokenize.casual.html\n",
    "# Difference between TweetTokenizer and Word_Tokenize: https://stackoverflow.com/questions/61919670/how-nltk-tweettokenizer-different-from-nltk-word-tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e1f2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstandard_nltk = word_tokenize(text)\\nprint(standard_nltk)\\n# output: [\\'The\\', \\'quick\\', \\'brown\\', \\'fox\\', \\'is\\', \"n\\'t\", \\'jumping\\', \\'over\\', \\n# \\'the\\', \\'lazy\\', \\'dog\\', \\',\\', \\'co-founder\\', \\'multi-word\\', \\'expression\\', \\'.\\', \\n# \\'#\\', \\'yes\\', \\'!\\']\\n\\ntwitter_nltk = tweet_tokenizer.tokenize(text)\\nprint(twitter_nltk)\\n# output: [\\'The\\', \\'quick\\', \\'brown\\', \\'fox\\', \"isn\\'t\", \\'jumping\\', \\'over\\', \\n# \\'the\\', \\'lazy\\', \\'dog\\', \\',\\', \\'co-founder\\', \\'multi-word\\', \\'expression\\', \\'.\\', \\n# \\'#yes\\', \\'!\\']\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "standard_nltk = word_tokenize(text)\n",
    "print(standard_nltk)\n",
    "# output: ['The', 'quick', 'brown', 'fox', 'is', \"n't\", 'jumping', 'over', \n",
    "# 'the', 'lazy', 'dog', ',', 'co-founder', 'multi-word', 'expression', '.', \n",
    "# '#', 'yes', '!']\n",
    "\n",
    "twitter_nltk = tweet_tokenizer.tokenize(text)\n",
    "print(twitter_nltk)\n",
    "# output: ['The', 'quick', 'brown', 'fox', \"isn't\", 'jumping', 'over', \n",
    "# 'the', 'lazy', 'dog', ',', 'co-founder', 'multi-word', 'expression', '.', \n",
    "# '#yes', '!']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa400d1f",
   "metadata": {},
   "source": [
    "We can try to remove stopwords and see how the model reacts, from **Symeonidis, Effrosynidis, and Arampatzis (2018). A comparative evaluation of pre-processing techniques and their interactions for twitter sentiment analysis**:\n",
    "\n",
    "The technique of removing stopwords yielded ambiguous results. For the SS-Twitter dataset, none of the algorithms was over the baseline accuracy but for the SemEval dataset, on three classic algorithms, the results were satisfactory. \n",
    "\n",
    "The reasons for failure are: first, stopwords like ‘I’, ‘me’, ‘you’, present and are associated with expressions of sentiment ( Thelwall et al., 2012 ),second, the domain of Tweets for each dataset, and third the vocabulary and the age of users. According to Haas et al. (2011) young people tend to use more and more short text with slangs and many stopwords to express their feelings about themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfa3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(text,lemmatizer=None, stemmer=None, remove_stopwords=None):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs and user mentions\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"URL\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"USER\", text)\n",
    "\n",
    "    # Expand contractions (we use contractions library for this)\n",
    "    # Contractions library Source: https://pypi.org/project/contractions/\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # # Replace numbers with [NUM]\n",
    "    # text = re.sub(r\"\\d+(\\.\\d+)?\", \"[NUM]\", text)\n",
    "\n",
    "    # Convert to tickers (e.g., $AAPL to [TICKER])\n",
    "    text = re.sub(r\"\\$[a-z]{1,5}\", \"[TICKER]\", text)\n",
    "\n",
    "    #Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize punctuation repetitions\n",
    "    text = re.sub(r\"([!?\\.])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Optionally remove stopwords and punctuation\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    else:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatization OR stemming (not both!)\n",
    "    if lemmatizer is not None and stemmer is None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif stemmer is not None and lemmatizer is None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    elif lemmatizer is not None and stemmer is not None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Else, leave tokens as is\n",
    "\n",
    "    # Source: https://www.nltk.org/api/nltk.tokenize.treebank.html \n",
    "    # TreebankWordDetokenizer from NLTK takes care of the correct spacing and formatting, \n",
    "    # we you get a well-formed sentence that looks like natural English (e.g. without TreebankWordDetokinzer: This is an example tweet ! , With: This is an example tweet!)\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701833c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you doing? I am doing fine!\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the contractions library\n",
    "test = \"Hello how're you doing? I'm doing fine!\"\n",
    "test_fix = contractions.fix(test)\n",
    "print(test_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bffaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train.copy()\n",
    "df_test_cleaned = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9333a9b",
   "metadata": {},
   "source": [
    "### Try the different combinations of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de6a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text_no_lemma_no_stem_with_stopwords...\n",
      "Processing text_lemma_no_stem_with_stopwords...\n",
      "Processing text_no_lemma_stem_with_stopwords...\n",
      "Processing text_no_lemma_no_stem_no_stopwords...\n",
      "Processing text_lemma_no_stem_no_stopwords...\n",
      "Processing text_no_lemma_stem_no_stopwords...\n",
      "Processing text_lemma_stem_with_stopwords...\n",
      "Processing text_lemma_stem_no_stopwords...\n",
      "Processing complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_no_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_no_lemma_stem_no_stopwords</th>\n",
       "      <th>text_lemma_stem_with_stopwords</th>\n",
       "      <th>text_lemma_stem_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER jpmorgan reels in expectations on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel in expectation on beyond meat URL</td>\n",
       "      <td>ticker jpmorgan reel in expect on beyond meat url</td>\n",
       "      <td>TICKER jpmorgan reels expectations beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel expectation beyond meat URL</td>\n",
       "      <td>ticker jpmorgan reel expect beyond meat url</td>\n",
       "      <td>TICKER jpmorgan reel in expectation on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel expectation beyond meat URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER TICKER nomura points to bookings weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL</td>\n",
       "      <td>ticker ticker nomura point to book weak at carniv and royal caribbean url</td>\n",
       "      <td>TICKER TICKER nomura points bookings weakness carnival royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point booking weakness carnival royal caribbean URL</td>\n",
       "      <td>ticker ticker nomura point book weak carniv royal caribbean url</td>\n",
       "      <td>TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point booking weakness carnival royal caribbean URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>ticker cemex cut at credit suiss j p morgan on weak build outlook url</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "      <td>ticker cemex cut credit suiss j p morgan weak build outlook url</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER]: btig research cuts to neutral URL</td>\n",
       "      <td>TICKER]: btig research cut to neutral URL</td>\n",
       "      <td>ticker]: btig research cut to neutral url</td>\n",
       "      <td>TICKER]: btig research cuts neutral URL</td>\n",
       "      <td>TICKER]: btig research cut neutral URL</td>\n",
       "      <td>ticker]: btig research cut neutral url</td>\n",
       "      <td>TICKER]: btig research cut to neutral URL</td>\n",
       "      <td>TICKER]: btig research cut neutral URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER funko slides after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide after piper jaffray pt cut URL</td>\n",
       "      <td>ticker funko slide after piper jaffray pt cut url</td>\n",
       "      <td>TICKER funko slides piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide piper jaffray pt cut URL</td>\n",
       "      <td>ticker funko slide piper jaffray pt cut url</td>\n",
       "      <td>TICKER funko slide after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide piper jaffray pt cut URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "0                           $BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT   \n",
       "1  $CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3   \n",
       "2          $CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb   \n",
       "3                                             $ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N   \n",
       "4                                 $FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                   text_no_lemma_no_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reels in expectations on beyond meat URL   \n",
       "1  TICKER TICKER nomura points to bookings weakness at carnival and royal caribbean URL   \n",
       "2             TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                            TICKER]: btig research cuts to neutral URL   \n",
       "4                                    TICKER funko slides after piper jaffray pt cut URL   \n",
       "\n",
       "                                                    text_lemma_no_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reel in expectation on beyond meat URL   \n",
       "1  TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL   \n",
       "2           TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                           TICKER]: btig research cut to neutral URL   \n",
       "4                                   TICKER funko slide after piper jaffray pt cut URL   \n",
       "\n",
       "                                           text_no_lemma_stem_with_stopwords  \\\n",
       "0                          ticker jpmorgan reel in expect on beyond meat url   \n",
       "1  ticker ticker nomura point to book weak at carniv and royal caribbean url   \n",
       "2      ticker cemex cut at credit suiss j p morgan on weak build outlook url   \n",
       "3                                  ticker]: btig research cut to neutral url   \n",
       "4                          ticker funko slide after piper jaffray pt cut url   \n",
       "\n",
       "                                           text_no_lemma_no_stem_no_stopwords  \\\n",
       "0                          TICKER jpmorgan reels expectations beyond meat URL   \n",
       "1  TICKER TICKER nomura points bookings weakness carnival royal caribbean URL   \n",
       "2         TICKER cemex cut credit suisse j p morgan weak building outlook URL   \n",
       "3                                     TICKER]: btig research cuts neutral URL   \n",
       "4                                TICKER funko slides piper jaffray pt cut URL   \n",
       "\n",
       "                                            text_lemma_no_stem_no_stopwords  \\\n",
       "0                          TICKER jpmorgan reel expectation beyond meat URL   \n",
       "1  TICKER TICKER nomura point booking weakness carnival royal caribbean URL   \n",
       "2       TICKER cemex cut credit suisse j p morgan weak building outlook URL   \n",
       "3                                    TICKER]: btig research cut neutral URL   \n",
       "4                               TICKER funko slide piper jaffray pt cut URL   \n",
       "\n",
       "                                   text_no_lemma_stem_no_stopwords  \\\n",
       "0                      ticker jpmorgan reel expect beyond meat url   \n",
       "1  ticker ticker nomura point book weak carniv royal caribbean url   \n",
       "2  ticker cemex cut credit suiss j p morgan weak build outlook url   \n",
       "3                           ticker]: btig research cut neutral url   \n",
       "4                      ticker funko slide piper jaffray pt cut url   \n",
       "\n",
       "                                                       text_lemma_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reel in expectation on beyond meat URL   \n",
       "1  TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL   \n",
       "2           TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                           TICKER]: btig research cut to neutral URL   \n",
       "4                                   TICKER funko slide after piper jaffray pt cut URL   \n",
       "\n",
       "                                               text_lemma_stem_no_stopwords  \n",
       "0                          TICKER jpmorgan reel expectation beyond meat URL  \n",
       "1  TICKER TICKER nomura point booking weakness carnival royal caribbean URL  \n",
       "2       TICKER cemex cut credit suisse j p morgan weak building outlook URL  \n",
       "3                                    TICKER]: btig research cut neutral URL  \n",
       "4                               TICKER funko slide piper jaffray pt cut URL  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the combinations to try\n",
    "combinations = [\n",
    "    {'lemmatizer': None, 'stemmer': None, 'remove_stopwords': False, 'name': 'no_lemma_no_stem_with_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': None, 'remove_stopwords': False, 'name': 'lemma_no_stem_with_stopwords'},\n",
    "    {'lemmatizer': None, 'stemmer': stemmer, 'remove_stopwords': False, 'name': 'no_lemma_stem_with_stopwords'},\n",
    "    {'lemmatizer': None, 'stemmer': None, 'remove_stopwords': True, 'name': 'no_lemma_no_stem_no_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': None, 'remove_stopwords': True, 'name': 'lemma_no_stem_no_stopwords'},\n",
    "    {'lemmatizer': None, 'stemmer': stemmer, 'remove_stopwords': True, 'name': 'no_lemma_stem_no_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': stemmer, 'remove_stopwords': False, 'name': 'lemma_stem_with_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': stemmer, 'remove_stopwords': True, 'name': 'lemma_stem_no_stopwords'}\n",
    "]\n",
    "\n",
    "# Process each combination and add to the dataframe\n",
    "for combo in combinations:\n",
    "    column_name = f\"text_{combo['name']}\"\n",
    "    print(f\"Processing {column_name}...\")\n",
    "    \n",
    "    # Apply the clean_text_column function with the current combination\n",
    "    df_train_cleaned[column_name] = df_train_cleaned['text'].apply(\n",
    "        lambda x: clean_text_column(\n",
    "            x, \n",
    "            lemmatizer=combo['lemmatizer'], \n",
    "            stemmer=combo['stemmer'], \n",
    "            remove_stopwords=combo['remove_stopwords']\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Also apply the best combination to the test set later after evaluation\n",
    "print(\"Processing complete\")\n",
    "\n",
    "# Display the first few rows with all the combinations\n",
    "df_train_cleaned.iloc[:10, :10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6627efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stratify to maintain the distribution of classes in the train, validation, and test sets\n",
    "train_df, val_test_df = train_test_split(df_train_cleaned, test_size=0.3, stratify=df_train_cleaned['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c68182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c591a",
   "metadata": {},
   "source": [
    "## 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf246b",
   "metadata": {},
   "source": [
    "Note: (to remove later) 3.3 Sentence Encoders (using Transformers for Embeddings only)\n",
    "Here, you use models like all-mpnet-base-v2, all-MiniLM-L6-v2, or USE, LASER, etc. to generate fixed-size sentence embeddings (vectors).\n",
    "\n",
    "After you get these embeddings, you feed them to traditional ML models (like logistic regression, SVM, XGB, LSTM, etc.).\n",
    "\n",
    "You do NOT fine-tune the transformer or use its classification head. You just use it as a feature extractor.\n",
    "\n",
    "✅ This fits 3.3, because your core model is not a transformer. The transformer is just making better features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404df49",
   "metadata": {},
   "source": [
    "We decided to follow a general pipeline, where based on the feature extraction technique we employ , and that it is adequate to the classification model we first define:\n",
    "\n",
    "- 3.1. - Statistical Methods: Bag of Words, and TF-IDF -> 3.1.1 Classification models: SVC, XGB, Logistic Regression and KNN -> 3.1.2 Hyperparamter Tuning for the best feature extraction technique and for the best model\n",
    "\n",
    "- 3.2. - Fixed Word Embedding Encoders -> Word2Vec, FastText , Glove-Twitter -> 3.2.1 Classification Models -> Keep the best traditional ML model from 3.1 and add BiLSTM , BiGRU , BiLSTM + Attention , BiGRU + Attention, CNN  (Source: https://sbert.net/docs/sentence_transformer/pretrained_models.html) \n",
    "\n",
    "- 3.3. - Contextual Word Embedding Encoders -> ELMO (mean and concat) -> 3.3.1 Classification Models -> Keep the best traditional ML model from 3.1 and add BiLSTM , BiGRU , BiLSTM + Attention , BiGRU + Attention, CNN \n",
    "\n",
    "- 3.4. - Sentence Encoders -> all-mpnet-base-v2 , all-distilroberta-v1 , all-MiniLM-L12-v2 , paraphrase-multilingual-mpnet-base-v2 -> 3.4.1 Classification Models -> Keep the best traditional ML model from 3.1 and add BiLSTM , BiGRU , BiLSTM + Attention , BiGRU + Attention, CNN\n",
    "\n",
    "- 3.5 -> Transformers -> BERT base, BERT Large, XLNET base, XLNET large, Roberta Base, Roberta Large distilbert large, distilbert base, ALBERT x large-v1 , ALBERT-xxlarge-v2 , XLM-MLM-en-2048 , BART-LARGE  \n",
    "\n",
    "- 3.6 -> Domain Specific Transformers: FinBert , BERTweet , FinTwitBERT (https://huggingface.co/StephanAkkerman/FinTwitBERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89b09b",
   "metadata": {},
   "source": [
    "### 3.1 - Statistical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382aa96",
   "metadata": {},
   "source": [
    "### 3.1.1 - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5886bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\n",
    "    (\"text_no_lemma_no_stem_with_stopwords\", train_df[\"text_no_lemma_no_stem_with_stopwords\"]),\n",
    "    (\"text_lemma_no_stem_with_stopwords\", train_df[\"text_lemma_no_stem_with_stopwords\"]),\n",
    "    (\"text_no_lemma_stem_with_stopwords\", train_df[\"text_no_lemma_stem_with_stopwords\"]),\n",
    "    (\"text_no_lemma_no_stem_no_stopwords\", train_df[\"text_no_lemma_no_stem_no_stopwords\"]),\n",
    "    (\"text_lemma_no_stem_no_stopwords\", train_df[\"text_lemma_no_stem_no_stopwords\"]),\n",
    "    (\"text_no_lemma_stem_no_stopwords\", train_df[\"text_no_lemma_stem_no_stopwords\"]),\n",
    "    (\"text_lemma_stem_with_stopwords\", train_df[\"text_lemma_stem_with_stopwords\"]),\n",
    "    (\"text_lemma_stem_no_stopwords\", train_df[\"text_lemma_stem_no_stopwords\"]),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3829f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting bow vectorizer for text_no_lemma_no_stem_with_stopwords...\n",
      "Fitting bow vectorizer for text_lemma_no_stem_with_stopwords...\n",
      "Fitting bow vectorizer for text_no_lemma_stem_with_stopwords...\n",
      "Fitting bow vectorizer for text_no_lemma_no_stem_no_stopwords...\n",
      "Fitting bow vectorizer for text_lemma_no_stem_no_stopwords...\n",
      "Fitting bow vectorizer for text_no_lemma_stem_no_stopwords...\n",
      "Fitting bow vectorizer for text_lemma_stem_with_stopwords...\n",
      "Fitting bow vectorizer for text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "# Fit vectorizer (BoW)\n",
    "bow_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=15_000)\n",
    "\n",
    "bow_vectors = {}\n",
    "for column_name, train_series in combinations:\n",
    "    print(f\"Fitting bow vectorizer for {column_name}...\")\n",
    "    \n",
    "    bow_vectorizer.fit(train_series)\n",
    "    X_train_bow = bow_vectorizer.transform(train_df[column_name])\n",
    "    X_val_bow = bow_vectorizer.transform(val_df[column_name])\n",
    "    X_test_bow = bow_vectorizer.transform(test_df[column_name])\n",
    "    \n",
    "    bow_vectors[column_name] = {\n",
    "        \"train\": X_train_bow,\n",
    "        \"val\": X_val_bow,\n",
    "        \"test\": X_test_bow,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e496d",
   "metadata": {},
   "source": [
    "### 3.1.2 - Traditional ML Classifiers using Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0b7b6",
   "metadata": {},
   "source": [
    "#### SVC, XGB , Logistic Regression and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da38d6",
   "metadata": {},
   "source": [
    "### 3.1.2.1 - Without Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b27f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SVC\": SVC(class_weight='balanced', random_state=42),  # Add class_weight for imbalanced data\n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight='balanced', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599bd568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_no_lemma_no_stem_with_stopwords ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bow_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m column_name = col[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Results for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_train = \u001b[43mbow_vectors\u001b[49m[column_name][\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m X_val = bow_vectors[column_name][\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# (add X_test as needed)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'bow_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "results_bow = []\n",
    "\n",
    "\n",
    "for col in combinations:\n",
    "    column_name = col[0]\n",
    "    print(f\"\\n=== Results for {column_name} ===\")\n",
    "    X_train = bow_vectors[column_name][\"train\"]\n",
    "    X_val = bow_vectors[column_name][\"val\"]\n",
    "    # (add X_test as needed)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # print(f\"\\n{name} - Validation set results:\")\n",
    "        # print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        results_bow.append({\n",
    "            \"variant\": column_name,\n",
    "            \"model\": name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"weighted_precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"weighted_recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        })\n",
    "\n",
    "traditional_ml_bow = pd.DataFrame(results_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61063861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.804333</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.733927</td>\n",
       "      <td>0.726412</td>\n",
       "      <td>0.803083</td>\n",
       "      <td>0.802119</td>\n",
       "      <td>0.804333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.804333</td>\n",
       "      <td>0.728305</td>\n",
       "      <td>0.734357</td>\n",
       "      <td>0.722844</td>\n",
       "      <td>0.802591</td>\n",
       "      <td>0.801212</td>\n",
       "      <td>0.804333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>0.724020</td>\n",
       "      <td>0.730592</td>\n",
       "      <td>0.718254</td>\n",
       "      <td>0.799584</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.801537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>0.724020</td>\n",
       "      <td>0.730592</td>\n",
       "      <td>0.718254</td>\n",
       "      <td>0.799584</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.801537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.796646</td>\n",
       "      <td>0.722437</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.721913</td>\n",
       "      <td>0.796537</td>\n",
       "      <td>0.796470</td>\n",
       "      <td>0.796646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789658</td>\n",
       "      <td>0.711204</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.708107</td>\n",
       "      <td>0.788621</td>\n",
       "      <td>0.787707</td>\n",
       "      <td>0.789658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789658</td>\n",
       "      <td>0.711204</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.708107</td>\n",
       "      <td>0.788621</td>\n",
       "      <td>0.787707</td>\n",
       "      <td>0.789658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.707967</td>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.703438</td>\n",
       "      <td>0.785430</td>\n",
       "      <td>0.784258</td>\n",
       "      <td>0.786862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant               model  accuracy  \\\n",
       "10     text_no_lemma_stem_with_stopwords  LogisticRegression  0.804333   \n",
       "2   text_no_lemma_no_stem_with_stopwords  LogisticRegression  0.804333   \n",
       "6      text_lemma_no_stem_with_stopwords  LogisticRegression  0.801537   \n",
       "26        text_lemma_stem_with_stopwords  LogisticRegression  0.801537   \n",
       "22       text_no_lemma_stem_no_stopwords  LogisticRegression  0.796646   \n",
       "18       text_lemma_no_stem_no_stopwords  LogisticRegression  0.789658   \n",
       "30          text_lemma_stem_no_stopwords  LogisticRegression  0.789658   \n",
       "14    text_no_lemma_no_stem_no_stopwords  LogisticRegression  0.786862   \n",
       "\n",
       "    macro_f1  macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "10  0.729893         0.733927      0.726412     0.803083            0.802119   \n",
       "2   0.728305         0.734357      0.722844     0.802591            0.801212   \n",
       "6   0.724020         0.730592      0.718254     0.799584            0.798097   \n",
       "26  0.724020         0.730592      0.718254     0.799584            0.798097   \n",
       "22  0.722437         0.723031      0.721913     0.796537            0.796470   \n",
       "18  0.711204         0.714500      0.708107     0.788621            0.787707   \n",
       "30  0.711204         0.714500      0.708107     0.788621            0.787707   \n",
       "14  0.707967         0.712828      0.703438     0.785430            0.784258   \n",
       "\n",
       "    weighted_recall  \n",
       "10         0.804333  \n",
       "2          0.804333  \n",
       "6          0.801537  \n",
       "26         0.801537  \n",
       "22         0.796646  \n",
       "18         0.789658  \n",
       "30         0.789658  \n",
       "14         0.786862  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_results = traditional_ml_bow[traditional_ml_bow['model'] == 'LogisticRegression']\n",
    "logistic_regression_results.sort_values(by='macro_f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb710b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691824</td>\n",
       "      <td>0.425190</td>\n",
       "      <td>0.774690</td>\n",
       "      <td>0.422158</td>\n",
       "      <td>0.608885</td>\n",
       "      <td>0.732466</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691824</td>\n",
       "      <td>0.424945</td>\n",
       "      <td>0.729050</td>\n",
       "      <td>0.422537</td>\n",
       "      <td>0.609742</td>\n",
       "      <td>0.710497</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.689727</td>\n",
       "      <td>0.423141</td>\n",
       "      <td>0.701842</td>\n",
       "      <td>0.421497</td>\n",
       "      <td>0.608681</td>\n",
       "      <td>0.697303</td>\n",
       "      <td>0.689727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.689727</td>\n",
       "      <td>0.423141</td>\n",
       "      <td>0.701842</td>\n",
       "      <td>0.421497</td>\n",
       "      <td>0.608681</td>\n",
       "      <td>0.697303</td>\n",
       "      <td>0.689727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.421091</td>\n",
       "      <td>0.770968</td>\n",
       "      <td>0.419825</td>\n",
       "      <td>0.606864</td>\n",
       "      <td>0.730511</td>\n",
       "      <td>0.691125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.421091</td>\n",
       "      <td>0.770968</td>\n",
       "      <td>0.419825</td>\n",
       "      <td>0.606864</td>\n",
       "      <td>0.730511</td>\n",
       "      <td>0.691125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.414729</td>\n",
       "      <td>0.679428</td>\n",
       "      <td>0.416129</td>\n",
       "      <td>0.603450</td>\n",
       "      <td>0.684915</td>\n",
       "      <td>0.686233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.687631</td>\n",
       "      <td>0.413012</td>\n",
       "      <td>0.768899</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.601183</td>\n",
       "      <td>0.728253</td>\n",
       "      <td>0.687631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant model  accuracy  macro_f1  \\\n",
       "11     text_no_lemma_stem_with_stopwords   KNN  0.691824  0.425190   \n",
       "23       text_no_lemma_stem_no_stopwords   KNN  0.691824  0.424945   \n",
       "19       text_lemma_no_stem_no_stopwords   KNN  0.689727  0.423141   \n",
       "31          text_lemma_stem_no_stopwords   KNN  0.689727  0.423141   \n",
       "7      text_lemma_no_stem_with_stopwords   KNN  0.691125  0.421091   \n",
       "27        text_lemma_stem_with_stopwords   KNN  0.691125  0.421091   \n",
       "15    text_no_lemma_no_stem_no_stopwords   KNN  0.686233  0.414729   \n",
       "3   text_no_lemma_no_stem_with_stopwords   KNN  0.687631  0.413012   \n",
       "\n",
       "    macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "11         0.774690      0.422158     0.608885            0.732466   \n",
       "23         0.729050      0.422537     0.609742            0.710497   \n",
       "19         0.701842      0.421497     0.608681            0.697303   \n",
       "31         0.701842      0.421497     0.608681            0.697303   \n",
       "7          0.770968      0.419825     0.606864            0.730511   \n",
       "27         0.770968      0.419825     0.606864            0.730511   \n",
       "15         0.679428      0.416129     0.603450            0.684915   \n",
       "3          0.768899      0.414456     0.601183            0.728253   \n",
       "\n",
       "    weighted_recall  \n",
       "11         0.691824  \n",
       "23         0.691824  \n",
       "19         0.689727  \n",
       "31         0.689727  \n",
       "7          0.691125  \n",
       "27         0.691125  \n",
       "15         0.686233  \n",
       "3          0.687631  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_results = traditional_ml_bow[traditional_ml_bow['model'] == 'KNN']\n",
    "KNN_results.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc52b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.795248</td>\n",
       "      <td>0.708807</td>\n",
       "      <td>0.733814</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.788879</td>\n",
       "      <td>0.786820</td>\n",
       "      <td>0.795248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.795248</td>\n",
       "      <td>0.708807</td>\n",
       "      <td>0.733814</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.788879</td>\n",
       "      <td>0.786820</td>\n",
       "      <td>0.795248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.793850</td>\n",
       "      <td>0.706168</td>\n",
       "      <td>0.731985</td>\n",
       "      <td>0.686963</td>\n",
       "      <td>0.787265</td>\n",
       "      <td>0.785231</td>\n",
       "      <td>0.793850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.703110</td>\n",
       "      <td>0.719451</td>\n",
       "      <td>0.691943</td>\n",
       "      <td>0.786712</td>\n",
       "      <td>0.785097</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.703110</td>\n",
       "      <td>0.719451</td>\n",
       "      <td>0.691943</td>\n",
       "      <td>0.786712</td>\n",
       "      <td>0.785097</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.687732</td>\n",
       "      <td>0.784155</td>\n",
       "      <td>0.782031</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.696358</td>\n",
       "      <td>0.722241</td>\n",
       "      <td>0.677824</td>\n",
       "      <td>0.781179</td>\n",
       "      <td>0.778831</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.783368</td>\n",
       "      <td>0.693948</td>\n",
       "      <td>0.713456</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.778399</td>\n",
       "      <td>0.777025</td>\n",
       "      <td>0.783368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant model  accuracy  macro_f1  \\\n",
       "4      text_lemma_no_stem_with_stopwords   SVC  0.795248  0.708807   \n",
       "24        text_lemma_stem_with_stopwords   SVC  0.795248  0.708807   \n",
       "8      text_no_lemma_stem_with_stopwords   SVC  0.793850  0.706168   \n",
       "16       text_lemma_no_stem_no_stopwords   SVC  0.791055  0.703110   \n",
       "28          text_lemma_stem_no_stopwords   SVC  0.791055  0.703110   \n",
       "20       text_no_lemma_stem_no_stopwords   SVC  0.788260  0.697505   \n",
       "0   text_no_lemma_no_stem_with_stopwords   SVC  0.788260  0.696358   \n",
       "12    text_no_lemma_no_stem_no_stopwords   SVC  0.783368  0.693948   \n",
       "\n",
       "    macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "4          0.733814      0.690534     0.788879            0.786820   \n",
       "24         0.733814      0.690534     0.788879            0.786820   \n",
       "8          0.731985      0.686963     0.787265            0.785231   \n",
       "16         0.719451      0.691943     0.786712            0.785097   \n",
       "28         0.719451      0.691943     0.786712            0.785097   \n",
       "20         0.710938      0.687732     0.784155            0.782031   \n",
       "0          0.722241      0.677824     0.781179            0.778831   \n",
       "12         0.713456      0.681265     0.778399            0.777025   \n",
       "\n",
       "    weighted_recall  \n",
       "4          0.795248  \n",
       "24         0.795248  \n",
       "8          0.793850  \n",
       "16         0.791055  \n",
       "28         0.791055  \n",
       "20         0.788260  \n",
       "0          0.788260  \n",
       "12         0.783368  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_results = traditional_ml_bow[traditional_ml_bow['model'] == 'SVC']\n",
    "SVC_results.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4829756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.803634</td>\n",
       "      <td>0.706439</td>\n",
       "      <td>0.793473</td>\n",
       "      <td>0.662359</td>\n",
       "      <td>0.788428</td>\n",
       "      <td>0.801250</td>\n",
       "      <td>0.803634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.684711</td>\n",
       "      <td>0.778643</td>\n",
       "      <td>0.641550</td>\n",
       "      <td>0.775179</td>\n",
       "      <td>0.788909</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.684711</td>\n",
       "      <td>0.778643</td>\n",
       "      <td>0.641550</td>\n",
       "      <td>0.775179</td>\n",
       "      <td>0.788909</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.683973</td>\n",
       "      <td>0.786771</td>\n",
       "      <td>0.637981</td>\n",
       "      <td>0.774109</td>\n",
       "      <td>0.791356</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.791754</td>\n",
       "      <td>0.680158</td>\n",
       "      <td>0.781548</td>\n",
       "      <td>0.635747</td>\n",
       "      <td>0.772481</td>\n",
       "      <td>0.788734</td>\n",
       "      <td>0.791754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.654559</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.612439</td>\n",
       "      <td>0.754866</td>\n",
       "      <td>0.773446</td>\n",
       "      <td>0.777079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.654559</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.612439</td>\n",
       "      <td>0.754866</td>\n",
       "      <td>0.773446</td>\n",
       "      <td>0.777079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.769392</td>\n",
       "      <td>0.643963</td>\n",
       "      <td>0.756890</td>\n",
       "      <td>0.601302</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.765352</td>\n",
       "      <td>0.769392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant model  accuracy  macro_f1  \\\n",
       "9      text_no_lemma_stem_with_stopwords   XGB  0.803634  0.706439   \n",
       "5      text_lemma_no_stem_with_stopwords   XGB  0.793152  0.684711   \n",
       "25        text_lemma_stem_with_stopwords   XGB  0.793152  0.684711   \n",
       "1   text_no_lemma_no_stem_with_stopwords   XGB  0.793152  0.683973   \n",
       "21       text_no_lemma_stem_no_stopwords   XGB  0.791754  0.680158   \n",
       "17       text_lemma_no_stem_no_stopwords   XGB  0.777079  0.654559   \n",
       "29          text_lemma_stem_no_stopwords   XGB  0.777079  0.654559   \n",
       "13    text_no_lemma_no_stem_no_stopwords   XGB  0.769392  0.643963   \n",
       "\n",
       "    macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "9          0.793473      0.662359     0.788428            0.801250   \n",
       "5          0.778643      0.641550     0.775179            0.788909   \n",
       "25         0.778643      0.641550     0.775179            0.788909   \n",
       "1          0.786771      0.637981     0.774109            0.791356   \n",
       "21         0.781548      0.635747     0.772481            0.788734   \n",
       "17         0.765019      0.612439     0.754866            0.773446   \n",
       "29         0.765019      0.612439     0.754866            0.773446   \n",
       "13         0.756890      0.601302     0.746000            0.765352   \n",
       "\n",
       "    weighted_recall  \n",
       "9          0.803634  \n",
       "5          0.793152  \n",
       "25         0.793152  \n",
       "1          0.793152  \n",
       "21         0.791754  \n",
       "17         0.777079  \n",
       "29         0.777079  \n",
       "13         0.769392  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_results = traditional_ml_bow[traditional_ml_bow['model'] == 'XGB']\n",
    "XGB_results.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6649d",
   "metadata": {},
   "source": [
    "### 3.1.2.2 - With Oversampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba059d1",
   "metadata": {},
   "source": [
    "### 3.1.3 - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2cee5530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tf idf vectorizer for text_no_lemma_no_stem_with_stopwords...\n",
      "Fitting tf idf vectorizer for text_lemma_no_stem_with_stopwords...\n",
      "Fitting tf idf vectorizer for text_no_lemma_stem_with_stopwords...\n",
      "Fitting tf idf vectorizer for text_no_lemma_no_stem_no_stopwords...\n",
      "Fitting tf idf vectorizer for text_lemma_no_stem_no_stopwords...\n",
      "Fitting tf idf vectorizer for text_no_lemma_stem_no_stopwords...\n",
      "Fitting tf idf vectorizer for text_lemma_stem_with_stopwords...\n",
      "Fitting tf idf vectorizer for text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=15_000)\n",
    "\n",
    "\n",
    "tfidf_vectors = {}\n",
    "\n",
    "for column_name, train_series in combinations:\n",
    "    print(f\"Fitting tf idf vectorizer for {column_name}...\")\n",
    "    \n",
    "    tfidf_vectorizer.fit(train_series)\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(train_df[column_name])\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(val_df[column_name])\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_df[column_name])\n",
    "    \n",
    "    tfidf_vectors[column_name] = {\n",
    "        \"train\": X_train_tfidf,\n",
    "        \"val\": X_val_tfidf,\n",
    "        \"test\": X_test_tfidf,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6db8a3",
   "metadata": {},
   "source": [
    "### 3.1.4 - Traditional ML Classifiers using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f639aa",
   "metadata": {},
   "source": [
    "### 3.1.4.1 - Without Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4629eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_no_lemma_no_stem_with_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:34:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_lemma_no_stem_with_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:35:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_no_lemma_stem_with_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:35:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_no_lemma_no_stem_no_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:36:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_lemma_no_stem_no_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:36:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_no_lemma_stem_no_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:36:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_lemma_stem_with_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:37:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n",
      "\n",
      "=== Results for text_lemma_stem_no_stopwords ===\n",
      "\n",
      "Training SVC...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:37:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training KNN...\n"
     ]
    }
   ],
   "source": [
    "results_tfidf = []\n",
    "\n",
    "\n",
    "for col in combinations:\n",
    "    column_name = col[0]\n",
    "    # print(f\"\\n=== Results for {column_name} ===\")\n",
    "    X_train = tfidf_vectors[column_name][\"train\"]\n",
    "    X_val = tfidf_vectors[column_name][\"val\"]\n",
    "    # (X_test if we needed)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # print(f\"\\nTraining {name}...\")\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # print(f\"\\n{name} - Validation set results:\")\n",
    "        # print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        results_tfidf.append({\n",
    "            \"variant\": column_name,\n",
    "            \"model\": name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"weighted_precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"weighted_recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        })\n",
    "\n",
    "traditional_ml_tfidf = pd.DataFrame(results_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f085e91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.726905</td>\n",
       "      <td>0.717443</td>\n",
       "      <td>0.737918</td>\n",
       "      <td>0.797449</td>\n",
       "      <td>0.801580</td>\n",
       "      <td>0.794549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.722747</td>\n",
       "      <td>0.714299</td>\n",
       "      <td>0.732550</td>\n",
       "      <td>0.793628</td>\n",
       "      <td>0.797305</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.787561</td>\n",
       "      <td>0.720361</td>\n",
       "      <td>0.710561</td>\n",
       "      <td>0.731926</td>\n",
       "      <td>0.790626</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.787561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.787561</td>\n",
       "      <td>0.720361</td>\n",
       "      <td>0.710561</td>\n",
       "      <td>0.731926</td>\n",
       "      <td>0.790626</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.787561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.781971</td>\n",
       "      <td>0.710177</td>\n",
       "      <td>0.700950</td>\n",
       "      <td>0.721531</td>\n",
       "      <td>0.785385</td>\n",
       "      <td>0.790555</td>\n",
       "      <td>0.781971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.779175</td>\n",
       "      <td>0.703476</td>\n",
       "      <td>0.695583</td>\n",
       "      <td>0.712576</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>0.786011</td>\n",
       "      <td>0.779175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.779175</td>\n",
       "      <td>0.703476</td>\n",
       "      <td>0.695583</td>\n",
       "      <td>0.712576</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>0.786011</td>\n",
       "      <td>0.779175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.772187</td>\n",
       "      <td>0.693545</td>\n",
       "      <td>0.686310</td>\n",
       "      <td>0.701839</td>\n",
       "      <td>0.775084</td>\n",
       "      <td>0.778840</td>\n",
       "      <td>0.772187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant               model  accuracy  \\\n",
       "10     text_no_lemma_stem_with_stopwords  LogisticRegression  0.794549   \n",
       "2   text_no_lemma_no_stem_with_stopwords  LogisticRegression  0.791055   \n",
       "6      text_lemma_no_stem_with_stopwords  LogisticRegression  0.787561   \n",
       "26        text_lemma_stem_with_stopwords  LogisticRegression  0.787561   \n",
       "22       text_no_lemma_stem_no_stopwords  LogisticRegression  0.781971   \n",
       "18       text_lemma_no_stem_no_stopwords  LogisticRegression  0.779175   \n",
       "30          text_lemma_stem_no_stopwords  LogisticRegression  0.779175   \n",
       "14    text_no_lemma_no_stem_no_stopwords  LogisticRegression  0.772187   \n",
       "\n",
       "    macro_f1  macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "10  0.726905         0.717443      0.737918     0.797449            0.801580   \n",
       "2   0.722747         0.714299      0.732550     0.793628            0.797305   \n",
       "6   0.720361         0.710561      0.731926     0.790626            0.795116   \n",
       "26  0.720361         0.710561      0.731926     0.790626            0.795116   \n",
       "22  0.710177         0.700950      0.721531     0.785385            0.790555   \n",
       "18  0.703476         0.695583      0.712576     0.782105            0.786011   \n",
       "30  0.703476         0.695583      0.712576     0.782105            0.786011   \n",
       "14  0.693545         0.686310      0.701839     0.775084            0.778840   \n",
       "\n",
       "    weighted_recall  \n",
       "10         0.794549  \n",
       "2          0.791055  \n",
       "6          0.787561  \n",
       "26         0.787561  \n",
       "22         0.781971  \n",
       "18         0.779175  \n",
       "30         0.779175  \n",
       "14         0.772187  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tfidf = traditional_ml_tfidf[traditional_ml_tfidf['model'] == 'LogisticRegression']\n",
    "logreg_tfidf.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6573545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.815514</td>\n",
       "      <td>0.731998</td>\n",
       "      <td>0.778293</td>\n",
       "      <td>0.703326</td>\n",
       "      <td>0.806934</td>\n",
       "      <td>0.808865</td>\n",
       "      <td>0.815514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.809224</td>\n",
       "      <td>0.718657</td>\n",
       "      <td>0.778195</td>\n",
       "      <td>0.686690</td>\n",
       "      <td>0.798557</td>\n",
       "      <td>0.803592</td>\n",
       "      <td>0.809224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.716831</td>\n",
       "      <td>0.769087</td>\n",
       "      <td>0.687642</td>\n",
       "      <td>0.796493</td>\n",
       "      <td>0.799723</td>\n",
       "      <td>0.806429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.716831</td>\n",
       "      <td>0.769087</td>\n",
       "      <td>0.687642</td>\n",
       "      <td>0.796493</td>\n",
       "      <td>0.799723</td>\n",
       "      <td>0.806429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.798742</td>\n",
       "      <td>0.702859</td>\n",
       "      <td>0.745125</td>\n",
       "      <td>0.678140</td>\n",
       "      <td>0.789097</td>\n",
       "      <td>0.789579</td>\n",
       "      <td>0.798742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.797345</td>\n",
       "      <td>0.702200</td>\n",
       "      <td>0.745910</td>\n",
       "      <td>0.676204</td>\n",
       "      <td>0.787496</td>\n",
       "      <td>0.788126</td>\n",
       "      <td>0.797345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.797345</td>\n",
       "      <td>0.702200</td>\n",
       "      <td>0.745910</td>\n",
       "      <td>0.676204</td>\n",
       "      <td>0.787496</td>\n",
       "      <td>0.788126</td>\n",
       "      <td>0.797345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.791754</td>\n",
       "      <td>0.690145</td>\n",
       "      <td>0.745385</td>\n",
       "      <td>0.663118</td>\n",
       "      <td>0.780304</td>\n",
       "      <td>0.784250</td>\n",
       "      <td>0.791754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant model  accuracy  macro_f1  \\\n",
       "8      text_no_lemma_stem_with_stopwords   SVC  0.815514  0.731998   \n",
       "0   text_no_lemma_no_stem_with_stopwords   SVC  0.809224  0.718657   \n",
       "4      text_lemma_no_stem_with_stopwords   SVC  0.806429  0.716831   \n",
       "24        text_lemma_stem_with_stopwords   SVC  0.806429  0.716831   \n",
       "20       text_no_lemma_stem_no_stopwords   SVC  0.798742  0.702859   \n",
       "16       text_lemma_no_stem_no_stopwords   SVC  0.797345  0.702200   \n",
       "28          text_lemma_stem_no_stopwords   SVC  0.797345  0.702200   \n",
       "12    text_no_lemma_no_stem_no_stopwords   SVC  0.791754  0.690145   \n",
       "\n",
       "    macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "8          0.778293      0.703326     0.806934            0.808865   \n",
       "0          0.778195      0.686690     0.798557            0.803592   \n",
       "4          0.769087      0.687642     0.796493            0.799723   \n",
       "24         0.769087      0.687642     0.796493            0.799723   \n",
       "20         0.745125      0.678140     0.789097            0.789579   \n",
       "16         0.745910      0.676204     0.787496            0.788126   \n",
       "28         0.745910      0.676204     0.787496            0.788126   \n",
       "12         0.745385      0.663118     0.780304            0.784250   \n",
       "\n",
       "    weighted_recall  \n",
       "8          0.815514  \n",
       "0          0.809224  \n",
       "4          0.806429  \n",
       "24         0.806429  \n",
       "20         0.798742  \n",
       "16         0.797345  \n",
       "28         0.797345  \n",
       "12         0.791754  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tfidf = traditional_ml_tfidf[traditional_ml_tfidf['model'] == 'SVC']\n",
    "svc_tfidf.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c84724bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.400782</td>\n",
       "      <td>0.832377</td>\n",
       "      <td>0.407397</td>\n",
       "      <td>0.594207</td>\n",
       "      <td>0.757238</td>\n",
       "      <td>0.686233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.400782</td>\n",
       "      <td>0.832377</td>\n",
       "      <td>0.407397</td>\n",
       "      <td>0.594207</td>\n",
       "      <td>0.757238</td>\n",
       "      <td>0.686233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.399420</td>\n",
       "      <td>0.798271</td>\n",
       "      <td>0.406317</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.739143</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.397212</td>\n",
       "      <td>0.806693</td>\n",
       "      <td>0.405141</td>\n",
       "      <td>0.591853</td>\n",
       "      <td>0.744217</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.681342</td>\n",
       "      <td>0.393211</td>\n",
       "      <td>0.735729</td>\n",
       "      <td>0.402485</td>\n",
       "      <td>0.589159</td>\n",
       "      <td>0.709517</td>\n",
       "      <td>0.681342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.681342</td>\n",
       "      <td>0.393211</td>\n",
       "      <td>0.735729</td>\n",
       "      <td>0.402485</td>\n",
       "      <td>0.589159</td>\n",
       "      <td>0.709517</td>\n",
       "      <td>0.681342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.680643</td>\n",
       "      <td>0.389669</td>\n",
       "      <td>0.753325</td>\n",
       "      <td>0.400570</td>\n",
       "      <td>0.587281</td>\n",
       "      <td>0.716970</td>\n",
       "      <td>0.680643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.677149</td>\n",
       "      <td>0.376276</td>\n",
       "      <td>0.719072</td>\n",
       "      <td>0.392809</td>\n",
       "      <td>0.579482</td>\n",
       "      <td>0.700264</td>\n",
       "      <td>0.677149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant model  accuracy  macro_f1  \\\n",
       "7      text_lemma_no_stem_with_stopwords   KNN  0.686233  0.400782   \n",
       "27        text_lemma_stem_with_stopwords   KNN  0.686233  0.400782   \n",
       "3   text_no_lemma_no_stem_with_stopwords   KNN  0.684137  0.399420   \n",
       "11     text_no_lemma_stem_with_stopwords   KNN  0.684137  0.397212   \n",
       "19       text_lemma_no_stem_no_stopwords   KNN  0.681342  0.393211   \n",
       "31          text_lemma_stem_no_stopwords   KNN  0.681342  0.393211   \n",
       "23       text_no_lemma_stem_no_stopwords   KNN  0.680643  0.389669   \n",
       "15    text_no_lemma_no_stem_no_stopwords   KNN  0.677149  0.376276   \n",
       "\n",
       "    macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "7          0.832377      0.407397     0.594207            0.757238   \n",
       "27         0.832377      0.407397     0.594207            0.757238   \n",
       "3          0.798271      0.406317     0.592871            0.739143   \n",
       "11         0.806693      0.405141     0.591853            0.744217   \n",
       "19         0.735729      0.402485     0.589159            0.709517   \n",
       "31         0.735729      0.402485     0.589159            0.709517   \n",
       "23         0.753325      0.400570     0.587281            0.716970   \n",
       "15         0.719072      0.392809     0.579482            0.700264   \n",
       "\n",
       "    weighted_recall  \n",
       "7          0.686233  \n",
       "27         0.686233  \n",
       "3          0.684137  \n",
       "11         0.684137  \n",
       "19         0.681342  \n",
       "31         0.681342  \n",
       "23         0.680643  \n",
       "15         0.677149  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_tfidf = traditional_ml_tfidf[traditional_ml_tfidf['model'] == 'KNN']\n",
    "knn_tfidf.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ef0937b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.687158</td>\n",
       "      <td>0.767445</td>\n",
       "      <td>0.646310</td>\n",
       "      <td>0.774816</td>\n",
       "      <td>0.785059</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.678494</td>\n",
       "      <td>0.770458</td>\n",
       "      <td>0.635840</td>\n",
       "      <td>0.769970</td>\n",
       "      <td>0.783068</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.678494</td>\n",
       "      <td>0.770458</td>\n",
       "      <td>0.635840</td>\n",
       "      <td>0.769970</td>\n",
       "      <td>0.783068</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.783368</td>\n",
       "      <td>0.667776</td>\n",
       "      <td>0.763118</td>\n",
       "      <td>0.625007</td>\n",
       "      <td>0.763354</td>\n",
       "      <td>0.777473</td>\n",
       "      <td>0.783368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.775681</td>\n",
       "      <td>0.666082</td>\n",
       "      <td>0.739270</td>\n",
       "      <td>0.628942</td>\n",
       "      <td>0.758951</td>\n",
       "      <td>0.766108</td>\n",
       "      <td>0.775681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.652027</td>\n",
       "      <td>0.740427</td>\n",
       "      <td>0.614012</td>\n",
       "      <td>0.755425</td>\n",
       "      <td>0.765978</td>\n",
       "      <td>0.776380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.652027</td>\n",
       "      <td>0.740427</td>\n",
       "      <td>0.614012</td>\n",
       "      <td>0.755425</td>\n",
       "      <td>0.765978</td>\n",
       "      <td>0.776380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.763802</td>\n",
       "      <td>0.631598</td>\n",
       "      <td>0.734190</td>\n",
       "      <td>0.592920</td>\n",
       "      <td>0.739667</td>\n",
       "      <td>0.754626</td>\n",
       "      <td>0.763802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant model  accuracy  macro_f1  \\\n",
       "9      text_no_lemma_stem_with_stopwords   XGB  0.791055  0.687158   \n",
       "5      text_lemma_no_stem_with_stopwords   XGB  0.788260  0.678494   \n",
       "25        text_lemma_stem_with_stopwords   XGB  0.788260  0.678494   \n",
       "1   text_no_lemma_no_stem_with_stopwords   XGB  0.783368  0.667776   \n",
       "21       text_no_lemma_stem_no_stopwords   XGB  0.775681  0.666082   \n",
       "17       text_lemma_no_stem_no_stopwords   XGB  0.776380  0.652027   \n",
       "29          text_lemma_stem_no_stopwords   XGB  0.776380  0.652027   \n",
       "13    text_no_lemma_no_stem_no_stopwords   XGB  0.763802  0.631598   \n",
       "\n",
       "    macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "9          0.767445      0.646310     0.774816            0.785059   \n",
       "5          0.770458      0.635840     0.769970            0.783068   \n",
       "25         0.770458      0.635840     0.769970            0.783068   \n",
       "1          0.763118      0.625007     0.763354            0.777473   \n",
       "21         0.739270      0.628942     0.758951            0.766108   \n",
       "17         0.740427      0.614012     0.755425            0.765978   \n",
       "29         0.740427      0.614012     0.755425            0.765978   \n",
       "13         0.734190      0.592920     0.739667            0.754626   \n",
       "\n",
       "    weighted_recall  \n",
       "9          0.791055  \n",
       "5          0.788260  \n",
       "25         0.788260  \n",
       "1          0.783368  \n",
       "21         0.775681  \n",
       "17         0.776380  \n",
       "29         0.776380  \n",
       "13         0.763802  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_tfidf = traditional_ml_tfidf[traditional_ml_tfidf['model'] == 'XGB']\n",
    "xgb_tfidf.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca5512",
   "metadata": {},
   "source": [
    "### 3.1.5 - Hyperparameter Optimization for the best model coming from BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a6c93",
   "metadata": {},
   "source": [
    "#### 1) BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78454da0",
   "metadata": {},
   "source": [
    "The best variant and model that results from BoW is: **text_no_lemma_stem_with_stopwords** with **LogisticRegression**\thaving a **0.729893 f1 macro average score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdc823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4860 candidates, totalling 14580 fits\n",
      "Best parameters: {'clf__C': 1, 'clf__max_iter': 200, 'clf__solver': 'saga', 'vect__binary': True, 'vect__max_df': 0.8, 'vect__max_features': 15000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}\n",
      "Best CV f1_macro score: 0.7265723241238174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe_bow = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'vect__max_features': [10000, 15000, 20000],\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'clf__max_iter': [200, 300, 500],\n",
    "    'clf__solver': ['lbfgs', 'saga'], #'saga' is better with large vocabularies and can be faster with sparse data.\n",
    "    'vect__binary': [False, True],\n",
    "    'vect__min_df': [1, 2, 5],\n",
    "    'vect__max_df': [0.8, 0.9, 1.0] \n",
    "}\n",
    "\n",
    "X_train_hyperparam = train_df['text_no_lemma_stem_with_stopwords']\n",
    "\n",
    "grid_search = GridSearchCV(pipe_bow, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train_hyperparam, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV f1_macro score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "591da1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set results after hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.627     0.590     0.608       217\n",
      "           1      0.701     0.726     0.713       288\n",
      "           2      0.881     0.883     0.882       926\n",
      "\n",
      "    accuracy                          0.807      1431\n",
      "   macro avg      0.736     0.733     0.734      1431\n",
      "weighted avg      0.806     0.807     0.806      1431\n",
      "\n",
      "Test set results after hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.609     0.583     0.596       216\n",
      "           1      0.702     0.727     0.714       289\n",
      "           2      0.875     0.874     0.874       927\n",
      "\n",
      "    accuracy                          0.800      1432\n",
      "   macro avg      0.729     0.728     0.728      1432\n",
      "weighted avg      0.800     0.800     0.800      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_tuned_bow = grid_search.best_estimator_\n",
    "# Evaluate the tuned model on the validation set\n",
    "X_val_hyperparam = val_df['text_no_lemma_stem_with_stopwords']\n",
    "y_val_pred = model_tuned_bow.predict(X_val_hyperparam)\n",
    "print(\"Validation set results after hyperparameter tuning:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=3))\n",
    "\n",
    "#Evaluate on the test set\n",
    "X_test_hyperparam = test_df['text_no_lemma_stem_with_stopwords']\n",
    "y_test_pred = model_tuned_bow.predict(X_test_hyperparam)\n",
    "print(\"Test set results after hyperparameter tuning:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35c9af",
   "metadata": {},
   "source": [
    "#### 2) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipe_tfidf = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid_tfidf = {\n",
    "    'vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'vect__max_features': [10000, 15000, 20000],\n",
    "    'vect__use_idf': [True, False],\n",
    "    'vect__smooth_idf': [True, False],\n",
    "    'vect__sublinear_tf': [True, False],\n",
    "    'vect__min_df': [1, 2, 5],\n",
    "    'vect__max_df': [0.8, 0.9, 1.0],\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'clf__max_iter': [200, 300, 500],\n",
    "    'clf__solver': ['lbfgs', 'saga'],\n",
    "}\n",
    "\n",
    "X_train_tfidf_hyperparam = train_df['text_no_lemma_stem_with_stopwords']\n",
    "\n",
    "grid_search_tfidf = GridSearchCV(pipe_tfidf, param_grid_tfidf, cv=3, scoring='f1_macro', n_jobs=-1, verbose=2)\n",
    "grid_search_tfidf.fit(X_train_hyperparam, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_tfidf.best_params_)\n",
    "print(\"Best CV f1_macro score:\", grid_search_tfidf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f2604",
   "metadata": {},
   "source": [
    "### 3.2 - Fixed Word Embedding Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a193c",
   "metadata": {},
   "source": [
    "### 3.2.1 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37a36458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec for text_no_lemma_no_stem_with_stopwords...\n",
      "Training Word2Vec for text_lemma_no_stem_with_stopwords...\n",
      "Training Word2Vec for text_no_lemma_stem_with_stopwords...\n",
      "Training Word2Vec for text_no_lemma_no_stem_no_stopwords...\n",
      "Training Word2Vec for text_lemma_no_stem_no_stopwords...\n",
      "Training Word2Vec for text_no_lemma_stem_no_stopwords...\n",
      "Training Word2Vec for text_lemma_stem_with_stopwords...\n",
      "Training Word2Vec for text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "# Same hyperparameters as in AStudy of Feature Extraction techniques for Sentiment Analysis -> to be tuned later\n",
    "# Source: https://link.springer.com/chapter/10.1007/978-981-13-1501-5_41\n",
    "\n",
    "\n",
    "w2v_vectors = {}\n",
    "w2v_models = {}\n",
    "vector_size = 100  # Set your embedding size\n",
    "\n",
    "for column_name, train_series in combinations:\n",
    "    print(f\"Training Word2Vec for {column_name}...\")\n",
    "    \n",
    "    # Tokenize the tweets (lists of tokens)\n",
    "    train_sentences = [tweet.split() for tweet in train_df[column_name]]\n",
    "    val_sentences = [tweet.split() for tweet in val_df[column_name]]\n",
    "    test_sentences = [tweet.split() for tweet in test_df[column_name]]\n",
    "\n",
    "    # Train Word2Vec on the train set for this variant\n",
    "    w2v_model = Word2Vec(sentences=train_sentences, vector_size=vector_size, window=10, min_count=1, workers=7)\n",
    "    \n",
    "    # Store the model for this variant\n",
    "    w2v_models[column_name] = w2v_model\n",
    "    \n",
    "    # Function to get sentence embeddings\n",
    "    def avg_vector(tokens, model, size):\n",
    "        valid = [t for t in tokens if t in model.wv]\n",
    "        return np.mean(model.wv[valid], axis=0) if valid else np.zeros(size)\n",
    "    \n",
    "    # Transform each split into sentence vectors\n",
    "    X_train_w2v = np.vstack([avg_vector(tokens, w2v_model, vector_size) for tokens in train_sentences])\n",
    "    X_val_w2v   = np.vstack([avg_vector(tokens, w2v_model, vector_size) for tokens in val_sentences])\n",
    "    X_test_w2v  = np.vstack([avg_vector(tokens, w2v_model, vector_size) for tokens in test_sentences])\n",
    "\n",
    "    w2v_vectors[column_name] = {\n",
    "        \"train\": X_train_w2v,\n",
    "        \"val\": X_val_w2v,\n",
    "        \"test\": X_test_w2v,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1e48a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6680, 100), (1431, 100), (1432, 100))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors['text_no_lemma_stem_with_stopwords']['train'].shape, w2v_vectors['text_no_lemma_stem_with_stopwords']['val'].shape, w2v_vectors['text_no_lemma_stem_with_stopwords']['test'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cc15d",
   "metadata": {},
   "source": [
    "### 3.2.2 - Classifiers using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d8cbd",
   "metadata": {},
   "source": [
    "SEE: https://github.com/f-data/finSENT/blob/master/models/models.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe7fdc",
   "metadata": {},
   "source": [
    "1) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_w2v = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight='balanced', random_state=42),\n",
    "    # \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf586f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_no_lemma_no_stem_with_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:35:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_lemma_no_stem_with_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_no_lemma_stem_with_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_no_lemma_no_stem_no_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_lemma_no_stem_no_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_no_lemma_stem_no_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_lemma_stem_with_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for text_lemma_stem_no_stopwords ===\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "results_log_reg_w2v = []\n",
    "\n",
    "\n",
    "for col in combinations:\n",
    "    column_name = col[0]\n",
    "    print(f\"\\n=== Results for {column_name} ===\")\n",
    "    X_train = w2v_vectors[column_name][\"train\"]\n",
    "    X_val = w2v_vectors[column_name][\"val\"]\n",
    "    # (add X_test as needed)\n",
    "\n",
    "    for name, model in models_w2v.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # print(f\"\\n{name} - Validation set results:\")\n",
    "        # print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        results_log_reg_w2v.append({\n",
    "            \"variant\": column_name,\n",
    "            \"model\": name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"weighted_precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"weighted_recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        })\n",
    "\n",
    "df_log_reg_w2v = pd.DataFrame(results_log_reg_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a4a8c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.482760</td>\n",
       "      <td>0.543980</td>\n",
       "      <td>0.472243</td>\n",
       "      <td>0.643369</td>\n",
       "      <td>0.636971</td>\n",
       "      <td>0.683438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.580014</td>\n",
       "      <td>0.473574</td>\n",
       "      <td>0.471797</td>\n",
       "      <td>0.484054</td>\n",
       "      <td>0.598479</td>\n",
       "      <td>0.625033</td>\n",
       "      <td>0.580014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.467652</td>\n",
       "      <td>0.465707</td>\n",
       "      <td>0.483296</td>\n",
       "      <td>0.587973</td>\n",
       "      <td>0.623073</td>\n",
       "      <td>0.566737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.464939</td>\n",
       "      <td>0.463591</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.585751</td>\n",
       "      <td>0.621245</td>\n",
       "      <td>0.563941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.672956</td>\n",
       "      <td>0.463860</td>\n",
       "      <td>0.522372</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.631418</td>\n",
       "      <td>0.623390</td>\n",
       "      <td>0.672956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.461937</td>\n",
       "      <td>0.505388</td>\n",
       "      <td>0.456848</td>\n",
       "      <td>0.628857</td>\n",
       "      <td>0.615968</td>\n",
       "      <td>0.668064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.561845</td>\n",
       "      <td>0.459819</td>\n",
       "      <td>0.460126</td>\n",
       "      <td>0.471087</td>\n",
       "      <td>0.583497</td>\n",
       "      <td>0.615974</td>\n",
       "      <td>0.561845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.457794</td>\n",
       "      <td>0.519038</td>\n",
       "      <td>0.452249</td>\n",
       "      <td>0.627285</td>\n",
       "      <td>0.619146</td>\n",
       "      <td>0.672257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.679944</td>\n",
       "      <td>0.451631</td>\n",
       "      <td>0.536439</td>\n",
       "      <td>0.449192</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.624721</td>\n",
       "      <td>0.679944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.675751</td>\n",
       "      <td>0.447985</td>\n",
       "      <td>0.518588</td>\n",
       "      <td>0.444979</td>\n",
       "      <td>0.623955</td>\n",
       "      <td>0.617442</td>\n",
       "      <td>0.675751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.665269</td>\n",
       "      <td>0.441382</td>\n",
       "      <td>0.507676</td>\n",
       "      <td>0.438323</td>\n",
       "      <td>0.616320</td>\n",
       "      <td>0.608831</td>\n",
       "      <td>0.665269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.662474</td>\n",
       "      <td>0.439761</td>\n",
       "      <td>0.491938</td>\n",
       "      <td>0.438977</td>\n",
       "      <td>0.614323</td>\n",
       "      <td>0.601629</td>\n",
       "      <td>0.662474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.519916</td>\n",
       "      <td>0.436164</td>\n",
       "      <td>0.445912</td>\n",
       "      <td>0.451342</td>\n",
       "      <td>0.551480</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.519916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.517121</td>\n",
       "      <td>0.433473</td>\n",
       "      <td>0.441517</td>\n",
       "      <td>0.448806</td>\n",
       "      <td>0.547976</td>\n",
       "      <td>0.602244</td>\n",
       "      <td>0.517121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.520615</td>\n",
       "      <td>0.432644</td>\n",
       "      <td>0.441165</td>\n",
       "      <td>0.445064</td>\n",
       "      <td>0.551053</td>\n",
       "      <td>0.601922</td>\n",
       "      <td>0.520615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.510832</td>\n",
       "      <td>0.423658</td>\n",
       "      <td>0.438764</td>\n",
       "      <td>0.434741</td>\n",
       "      <td>0.544890</td>\n",
       "      <td>0.602953</td>\n",
       "      <td>0.510832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant               model  accuracy  \\\n",
       "5      text_no_lemma_stem_with_stopwords                 XGB  0.683438   \n",
       "4      text_no_lemma_stem_with_stopwords  LogisticRegression  0.580014   \n",
       "2      text_lemma_no_stem_with_stopwords  LogisticRegression  0.566737   \n",
       "12        text_lemma_stem_with_stopwords  LogisticRegression  0.563941   \n",
       "3      text_lemma_no_stem_with_stopwords                 XGB  0.672956   \n",
       "1   text_no_lemma_no_stem_with_stopwords                 XGB  0.668064   \n",
       "0   text_no_lemma_no_stem_with_stopwords  LogisticRegression  0.561845   \n",
       "13        text_lemma_stem_with_stopwords                 XGB  0.672257   \n",
       "7     text_no_lemma_no_stem_no_stopwords                 XGB  0.679944   \n",
       "11       text_no_lemma_stem_no_stopwords                 XGB  0.675751   \n",
       "15          text_lemma_stem_no_stopwords                 XGB  0.665269   \n",
       "9        text_lemma_no_stem_no_stopwords                 XGB  0.662474   \n",
       "8        text_lemma_no_stem_no_stopwords  LogisticRegression  0.519916   \n",
       "14          text_lemma_stem_no_stopwords  LogisticRegression  0.517121   \n",
       "10       text_no_lemma_stem_no_stopwords  LogisticRegression  0.520615   \n",
       "6     text_no_lemma_no_stem_no_stopwords  LogisticRegression  0.510832   \n",
       "\n",
       "    macro_f1  macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "5   0.482760         0.543980      0.472243     0.643369            0.636971   \n",
       "4   0.473574         0.471797      0.484054     0.598479            0.625033   \n",
       "2   0.467652         0.465707      0.483296     0.587973            0.623073   \n",
       "12  0.464939         0.463591      0.479842     0.585751            0.621245   \n",
       "3   0.463860         0.522372      0.457733     0.631418            0.623390   \n",
       "1   0.461937         0.505388      0.456848     0.628857            0.615968   \n",
       "0   0.459819         0.460126      0.471087     0.583497            0.615974   \n",
       "13  0.457794         0.519038      0.452249     0.627285            0.619146   \n",
       "7   0.451631         0.536439      0.449192     0.625759            0.624721   \n",
       "11  0.447985         0.518588      0.444979     0.623955            0.617442   \n",
       "15  0.441382         0.507676      0.438323     0.616320            0.608831   \n",
       "9   0.439761         0.491938      0.438977     0.614323            0.601629   \n",
       "8   0.436164         0.445912      0.451342     0.551480            0.606838   \n",
       "14  0.433473         0.441517      0.448806     0.547976            0.602244   \n",
       "10  0.432644         0.441165      0.445064     0.551053            0.601922   \n",
       "6   0.423658         0.438764      0.434741     0.544890            0.602953   \n",
       "\n",
       "    weighted_recall  \n",
       "5          0.683438  \n",
       "4          0.580014  \n",
       "2          0.566737  \n",
       "12         0.563941  \n",
       "3          0.672956  \n",
       "1          0.668064  \n",
       "0          0.561845  \n",
       "13         0.672257  \n",
       "7          0.679944  \n",
       "11         0.675751  \n",
       "15         0.665269  \n",
       "9          0.662474  \n",
       "8          0.519916  \n",
       "14         0.517121  \n",
       "10         0.520615  \n",
       "6          0.510832  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_reg_w2v.sort_values(by='macro_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b094ab",
   "metadata": {},
   "source": [
    "2) BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "vector_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70016b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_sequence(tweet, w2v_model, vector_size, maxlen):\n",
    "    tokens = tweet.split()\n",
    "    seq = []\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            seq.append(w2v_model.wv[token])\n",
    "        else:\n",
    "            seq.append(np.zeros(vector_size))\n",
    "    # Pad or truncate\n",
    "    if len(seq) < maxlen:\n",
    "        seq += [np.zeros(vector_size)] * (maxlen - len(seq))\n",
    "    else:\n",
    "        seq = seq[:maxlen]\n",
    "    return np.array(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2fdfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bilstm(units, input_length, embed_size):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Bidirectional(LSTM(units, return_sequences=False, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    out = Dense(3, activation='softmax')(x) # nr of classes \n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf9c09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BiLSTM input arrays for text_no_lemma_no_stem_with_stopwords...\n",
      "Building BiLSTM input arrays for text_lemma_no_stem_with_stopwords...\n",
      "Building BiLSTM input arrays for text_no_lemma_stem_with_stopwords...\n",
      "Building BiLSTM input arrays for text_no_lemma_no_stem_no_stopwords...\n",
      "Building BiLSTM input arrays for text_lemma_no_stem_no_stopwords...\n",
      "Building BiLSTM input arrays for text_no_lemma_stem_no_stopwords...\n",
      "Building BiLSTM input arrays for text_lemma_stem_with_stopwords...\n",
      "Building BiLSTM input arrays for text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "bilstm_inputs = {}\n",
    "\n",
    "for column_name, train_series in combinations:\n",
    "    print(f\"Building BiLSTM input arrays for {column_name}...\")\n",
    "\n",
    "    # Load or train Word2Vec for this variant\n",
    "    w2v_model = w2v_models[column_name]\n",
    "\n",
    "    # Prepare the sequences for each split\n",
    "    X_train_bilstm = np.stack([tweet_to_sequence(tweet, w2v_model, vector_size, max_len) for tweet in train_df[column_name]])\n",
    "    X_val_bilstm   = np.stack([tweet_to_sequence(tweet, w2v_model, vector_size, max_len) for tweet in val_df[column_name]])\n",
    "    X_test_bilstm  = np.stack([tweet_to_sequence(tweet, w2v_model, vector_size, max_len) for tweet in test_df[column_name]])\n",
    "\n",
    "    # Store\n",
    "    bilstm_inputs[column_name] = {\n",
    "        \"train\": X_train_bilstm,\n",
    "        \"val\": X_val_bilstm,\n",
    "        \"test\": X_test_bilstm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e784f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose our variant\n",
    "column_name = \"text_no_lemma_stem_with_stopwords\"\n",
    "\n",
    "# Get data for that variant\n",
    "X_train = bilstm_inputs[column_name][\"train\"]\n",
    "X_val   = bilstm_inputs[column_name][\"val\"]\n",
    "X_test  = bilstm_inputs[column_name][\"test\"]\n",
    "\n",
    "# Convert your y labels to categorical (one-hot)\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_val_cat   = to_categorical(y_val, num_classes=3)\n",
    "y_test_cat  = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Build the BiLSTM model\n",
    "model = build_bilstm(\n",
    "    units=64,\n",
    "    input_length=X_train.shape[1],  # Should be maxlen\n",
    "    embed_size=X_train.shape[2]     # Should be vector_size from w2v (e.g., 100)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=30,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "val_preds = np.argmax(model.predict(X_val), axis=1)\n",
    "print(classification_report(y_val, val_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06974f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM for text_no_lemma_no_stem_with_stopwords...\n",
      "Epoch 1/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.6314 - loss: 0.9167 - val_accuracy: 0.6471 - val_loss: 0.8820\n",
      "Epoch 2/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 90ms/step - accuracy: 0.6535 - loss: 0.8629 - val_accuracy: 0.6527 - val_loss: 0.8568\n",
      "Epoch 3/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.6412 - loss: 0.8715 - val_accuracy: 0.6541 - val_loss: 0.8548\n",
      "Epoch 4/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step - accuracy: 0.6642 - loss: 0.8365 - val_accuracy: 0.6632 - val_loss: 0.8444\n",
      "Epoch 5/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.6603 - loss: 0.8474 - val_accuracy: 0.6604 - val_loss: 0.8402\n",
      "Epoch 6/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 117ms/step - accuracy: 0.6653 - loss: 0.8257 - val_accuracy: 0.6646 - val_loss: 0.8371\n",
      "Epoch 7/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 120ms/step - accuracy: 0.6628 - loss: 0.8235 - val_accuracy: 0.6674 - val_loss: 0.8332\n",
      "Epoch 8/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 103ms/step - accuracy: 0.6615 - loss: 0.8269 - val_accuracy: 0.6653 - val_loss: 0.8256\n",
      "Epoch 9/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6549 - loss: 0.8264 - val_accuracy: 0.6667 - val_loss: 0.8262\n",
      "Epoch 10/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - accuracy: 0.6638 - loss: 0.8172 - val_accuracy: 0.6660 - val_loss: 0.8341\n",
      "Epoch 11/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 101ms/step - accuracy: 0.6670 - loss: 0.8171 - val_accuracy: 0.6653 - val_loss: 0.8208\n",
      "Epoch 12/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 99ms/step - accuracy: 0.6728 - loss: 0.8037 - val_accuracy: 0.6667 - val_loss: 0.8198\n",
      "Epoch 13/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.6664 - loss: 0.8042 - val_accuracy: 0.6709 - val_loss: 0.8130\n",
      "Epoch 14/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - accuracy: 0.6841 - loss: 0.7884 - val_accuracy: 0.6778 - val_loss: 0.8112\n",
      "Epoch 15/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 109ms/step - accuracy: 0.6628 - loss: 0.8110 - val_accuracy: 0.6792 - val_loss: 0.8093\n",
      "Epoch 16/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 102ms/step - accuracy: 0.6704 - loss: 0.8060 - val_accuracy: 0.6667 - val_loss: 0.8139\n",
      "Epoch 17/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 102ms/step - accuracy: 0.6760 - loss: 0.7898 - val_accuracy: 0.6674 - val_loss: 0.8165\n",
      "Epoch 18/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 107ms/step - accuracy: 0.6607 - loss: 0.7975 - val_accuracy: 0.6785 - val_loss: 0.8017\n",
      "Epoch 19/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 105ms/step - accuracy: 0.6770 - loss: 0.7824 - val_accuracy: 0.6841 - val_loss: 0.7992\n",
      "Epoch 20/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 109ms/step - accuracy: 0.6877 - loss: 0.7715 - val_accuracy: 0.6695 - val_loss: 0.8155\n",
      "Epoch 21/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 113ms/step - accuracy: 0.6833 - loss: 0.7773 - val_accuracy: 0.6806 - val_loss: 0.7960\n",
      "Epoch 22/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 102ms/step - accuracy: 0.6875 - loss: 0.7725 - val_accuracy: 0.6799 - val_loss: 0.8006\n",
      "Epoch 23/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 105ms/step - accuracy: 0.6803 - loss: 0.7761 - val_accuracy: 0.6855 - val_loss: 0.7960\n",
      "Epoch 24/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step - accuracy: 0.6813 - loss: 0.7703 - val_accuracy: 0.6771 - val_loss: 0.7944\n",
      "Epoch 25/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 103ms/step - accuracy: 0.6772 - loss: 0.7788 - val_accuracy: 0.6834 - val_loss: 0.7935\n",
      "Epoch 26/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 103ms/step - accuracy: 0.6802 - loss: 0.7690 - val_accuracy: 0.6827 - val_loss: 0.7960\n",
      "Epoch 27/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 101ms/step - accuracy: 0.6820 - loss: 0.7726 - val_accuracy: 0.6799 - val_loss: 0.7978\n",
      "Epoch 28/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 102ms/step - accuracy: 0.6899 - loss: 0.7626 - val_accuracy: 0.6806 - val_loss: 0.7927\n",
      "Epoch 29/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 104ms/step - accuracy: 0.6821 - loss: 0.7782 - val_accuracy: 0.6730 - val_loss: 0.8136\n",
      "Epoch 30/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step - accuracy: 0.6893 - loss: 0.7612 - val_accuracy: 0.6841 - val_loss: 0.7871\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.005     0.009       217\n",
      "           1      0.538     0.295     0.381       288\n",
      "           2      0.703     0.964     0.813       926\n",
      "\n",
      "    accuracy                          0.684      1431\n",
      "   macro avg      0.580     0.421     0.401      1431\n",
      "weighted avg      0.639     0.684     0.604      1431\n",
      "\n",
      "Training BiLSTM for text_lemma_no_stem_with_stopwords...\n",
      "Epoch 1/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 139ms/step - accuracy: 0.6481 - loss: 0.9091 - val_accuracy: 0.6457 - val_loss: 0.8692\n",
      "Epoch 2/30\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 119ms/step - accuracy: 0.6512 - loss: 0.8638 - val_accuracy: 0.6541 - val_loss: 0.8528\n",
      "Epoch 3/30\n",
      "\u001b[1m 59/105\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 0.6656 - loss: 0.8361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     12\u001b[39m model = build_bilstm(\n\u001b[32m     13\u001b[39m     units=\u001b[32m64\u001b[39m,\n\u001b[32m     14\u001b[39m     input_length=X_train.shape[\u001b[32m1\u001b[39m], \u001b[38;5;66;03m#max_len,\u001b[39;00m\n\u001b[32m     15\u001b[39m     embed_size=X_train.shape[\u001b[32m2\u001b[39m]   \u001b[38;5;66;03m# vector size from Word2Vec, e.g. 100\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_cat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[32m     22\u001b[39m val_preds = np.argmax(model.predict(X_val), axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# for column_name in bilstm_inputs:\n",
    "#     print(f\"Training BiLSTM for {column_name}...\")\n",
    "#     X_train = bilstm_inputs[column_name][\"train\"]\n",
    "#     X_val = bilstm_inputs[column_name][\"val\"]\n",
    "#     X_test = bilstm_inputs[column_name][\"test\"]\n",
    "\n",
    "#     # Assume y_train, y_val, y_test already exist as integer class labels\n",
    "#     y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "#     y_val_cat = to_categorical(y_val, num_classes=3)\n",
    "#     y_test_cat = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "#     model = build_bilstm(\n",
    "#         units=64,\n",
    "#         input_length=X_train.shape[1], #max_len,\n",
    "#         embed_size=X_train.shape[2]   # vector size from Word2Vec, e.g. 100\n",
    "#     )\n",
    "\n",
    "#     # Train\n",
    "#     model.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), epochs=30, batch_size=64)\n",
    "\n",
    "#     # Predict and evaluate\n",
    "#     val_preds = np.argmax(model.predict(X_val), axis=1)\n",
    "#     print(classification_report(y_val, val_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
