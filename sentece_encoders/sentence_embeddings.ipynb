{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8956595",
   "metadata": {},
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b397618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hugof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hugof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hugof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hugof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Text extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import contractions\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set pd options to display all columns and rows\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text without truncation\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6e4eb",
   "metadata": {},
   "source": [
    "## 1 - EDA (missing, just copy the other notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d86ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (where the notebook is)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct full paths to the CSV files\n",
    "train_path = os.path.join(BASE_DIR, \"data\", \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"data\", \"test.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e731793",
   "metadata": {},
   "source": [
    "## 2 - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd74cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Source: https://www.nltk.org/api/nltk.tokenize.casual.html\n",
    "# Difference between TweetTokenizer and Word_Tokenize: https://stackoverflow.com/questions/61919670/how-nltk-tweettokenizer-different-from-nltk-word-tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7840339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(text,lemmatizer=None, stemmer=None, remove_stopwords=None):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs and user mentions\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"URL\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"USER\", text)\n",
    "\n",
    "    # Expand contractions (we use contractions library for this)\n",
    "    # Contractions library Source: https://pypi.org/project/contractions/\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # # Replace numbers with [NUM]\n",
    "    # text = re.sub(r\"\\d+(\\.\\d+)?\", \"[NUM]\", text)\n",
    "\n",
    "    # Convert to tickers (e.g., $AAPL to [TICKER])\n",
    "    text = re.sub(r\"\\$[a-z]{1,5}\", \"[TICKER]\", text)\n",
    "\n",
    "    #Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize punctuation repetitions\n",
    "    text = re.sub(r\"([!?\\.])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Optionally remove stopwords and punctuation\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    else:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatization OR stemming (not both!)\n",
    "    if lemmatizer is not None and stemmer is None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif stemmer is not None and lemmatizer is None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    elif lemmatizer is not None and stemmer is not None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Else, leave tokens as is\n",
    "\n",
    "    # Source: https://www.nltk.org/api/nltk.tokenize.treebank.html \n",
    "    # TreebankWordDetokenizer from NLTK takes care of the correct spacing and formatting, \n",
    "    # we you get a well-formed sentence that looks like natural English (e.g. without TreebankWordDetokinzer: This is an example tweet ! , With: This is an example tweet!)\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a7fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train.copy()\n",
    "df_test_cleaned = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fca8eb",
   "metadata": {},
   "source": [
    "### Try the different combinations of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb8e4d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text_no_lemma_no_stem_with_stopwords...\n",
      "Processing text_lemma_no_stem_with_stopwords...\n",
      "Processing text_no_lemma_stem_with_stopwords...\n",
      "Processing text_no_lemma_no_stem_no_stopwords...\n",
      "Processing text_lemma_no_stem_no_stopwords...\n",
      "Processing text_no_lemma_stem_no_stopwords...\n",
      "Processing text_lemma_stem_with_stopwords...\n",
      "Processing text_lemma_stem_no_stopwords...\n",
      "Processing complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_no_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_no_lemma_stem_no_stopwords</th>\n",
       "      <th>text_lemma_stem_with_stopwords</th>\n",
       "      <th>text_lemma_stem_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER jpmorgan reels in expectations on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel in expectation on beyond meat URL</td>\n",
       "      <td>ticker jpmorgan reel in expect on beyond meat url</td>\n",
       "      <td>TICKER jpmorgan reels expectations beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel expectation beyond meat URL</td>\n",
       "      <td>ticker jpmorgan reel expect beyond meat url</td>\n",
       "      <td>TICKER jpmorgan reel in expectation on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel expectation beyond meat URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER TICKER nomura points to bookings weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL</td>\n",
       "      <td>ticker ticker nomura point to book weak at carniv and royal caribbean url</td>\n",
       "      <td>TICKER TICKER nomura points bookings weakness carnival royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point booking weakness carnival royal caribbean URL</td>\n",
       "      <td>ticker ticker nomura point book weak carniv royal caribbean url</td>\n",
       "      <td>TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point booking weakness carnival royal caribbean URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>ticker cemex cut at credit suiss j p morgan on weak build outlook url</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "      <td>ticker cemex cut credit suiss j p morgan weak build outlook url</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER]: btig research cuts to neutral URL</td>\n",
       "      <td>TICKER]: btig research cut to neutral URL</td>\n",
       "      <td>ticker]: btig research cut to neutral url</td>\n",
       "      <td>TICKER]: btig research cuts neutral URL</td>\n",
       "      <td>TICKER]: btig research cut neutral URL</td>\n",
       "      <td>ticker]: btig research cut neutral url</td>\n",
       "      <td>TICKER]: btig research cut to neutral URL</td>\n",
       "      <td>TICKER]: btig research cut neutral URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER funko slides after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide after piper jaffray pt cut URL</td>\n",
       "      <td>ticker funko slide after piper jaffray pt cut url</td>\n",
       "      <td>TICKER funko slides piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide piper jaffray pt cut URL</td>\n",
       "      <td>ticker funko slide piper jaffray pt cut url</td>\n",
       "      <td>TICKER funko slide after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide piper jaffray pt cut URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "0                           $BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT   \n",
       "1  $CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3   \n",
       "2          $CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb   \n",
       "3                                             $ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N   \n",
       "4                                 $FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                   text_no_lemma_no_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reels in expectations on beyond meat URL   \n",
       "1  TICKER TICKER nomura points to bookings weakness at carnival and royal caribbean URL   \n",
       "2             TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                            TICKER]: btig research cuts to neutral URL   \n",
       "4                                    TICKER funko slides after piper jaffray pt cut URL   \n",
       "\n",
       "                                                    text_lemma_no_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reel in expectation on beyond meat URL   \n",
       "1  TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL   \n",
       "2           TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                           TICKER]: btig research cut to neutral URL   \n",
       "4                                   TICKER funko slide after piper jaffray pt cut URL   \n",
       "\n",
       "                                           text_no_lemma_stem_with_stopwords  \\\n",
       "0                          ticker jpmorgan reel in expect on beyond meat url   \n",
       "1  ticker ticker nomura point to book weak at carniv and royal caribbean url   \n",
       "2      ticker cemex cut at credit suiss j p morgan on weak build outlook url   \n",
       "3                                  ticker]: btig research cut to neutral url   \n",
       "4                          ticker funko slide after piper jaffray pt cut url   \n",
       "\n",
       "                                           text_no_lemma_no_stem_no_stopwords  \\\n",
       "0                          TICKER jpmorgan reels expectations beyond meat URL   \n",
       "1  TICKER TICKER nomura points bookings weakness carnival royal caribbean URL   \n",
       "2         TICKER cemex cut credit suisse j p morgan weak building outlook URL   \n",
       "3                                     TICKER]: btig research cuts neutral URL   \n",
       "4                                TICKER funko slides piper jaffray pt cut URL   \n",
       "\n",
       "                                            text_lemma_no_stem_no_stopwords  \\\n",
       "0                          TICKER jpmorgan reel expectation beyond meat URL   \n",
       "1  TICKER TICKER nomura point booking weakness carnival royal caribbean URL   \n",
       "2       TICKER cemex cut credit suisse j p morgan weak building outlook URL   \n",
       "3                                    TICKER]: btig research cut neutral URL   \n",
       "4                               TICKER funko slide piper jaffray pt cut URL   \n",
       "\n",
       "                                   text_no_lemma_stem_no_stopwords  \\\n",
       "0                      ticker jpmorgan reel expect beyond meat url   \n",
       "1  ticker ticker nomura point book weak carniv royal caribbean url   \n",
       "2  ticker cemex cut credit suiss j p morgan weak build outlook url   \n",
       "3                           ticker]: btig research cut neutral url   \n",
       "4                      ticker funko slide piper jaffray pt cut url   \n",
       "\n",
       "                                                       text_lemma_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reel in expectation on beyond meat URL   \n",
       "1  TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL   \n",
       "2           TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                           TICKER]: btig research cut to neutral URL   \n",
       "4                                   TICKER funko slide after piper jaffray pt cut URL   \n",
       "\n",
       "                                               text_lemma_stem_no_stopwords  \n",
       "0                          TICKER jpmorgan reel expectation beyond meat URL  \n",
       "1  TICKER TICKER nomura point booking weakness carnival royal caribbean URL  \n",
       "2       TICKER cemex cut credit suisse j p morgan weak building outlook URL  \n",
       "3                                    TICKER]: btig research cut neutral URL  \n",
       "4                               TICKER funko slide piper jaffray pt cut URL  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the combinations to try\n",
    "combinations = [\n",
    "    {'lemmatizer': None, 'stemmer': None, 'remove_stopwords': False, 'name': 'no_lemma_no_stem_with_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': None, 'remove_stopwords': False, 'name': 'lemma_no_stem_with_stopwords'},\n",
    "    {'lemmatizer': None, 'stemmer': stemmer, 'remove_stopwords': False, 'name': 'no_lemma_stem_with_stopwords'},\n",
    "    {'lemmatizer': None, 'stemmer': None, 'remove_stopwords': True, 'name': 'no_lemma_no_stem_no_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': None, 'remove_stopwords': True, 'name': 'lemma_no_stem_no_stopwords'},\n",
    "    {'lemmatizer': None, 'stemmer': stemmer, 'remove_stopwords': True, 'name': 'no_lemma_stem_no_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': stemmer, 'remove_stopwords': False, 'name': 'lemma_stem_with_stopwords'},\n",
    "    {'lemmatizer': lemmatizer, 'stemmer': stemmer, 'remove_stopwords': True, 'name': 'lemma_stem_no_stopwords'}\n",
    "]\n",
    "\n",
    "# Process each combination and add to the dataframe\n",
    "for combo in combinations:\n",
    "    column_name = f\"text_{combo['name']}\"\n",
    "    print(f\"Processing {column_name}...\")\n",
    "    \n",
    "    # Apply the clean_text_column function with the current combination\n",
    "    df_train_cleaned[column_name] = df_train_cleaned['text'].apply(\n",
    "        lambda x: clean_text_column(\n",
    "            x, \n",
    "            lemmatizer=combo['lemmatizer'], \n",
    "            stemmer=combo['stemmer'], \n",
    "            remove_stopwords=combo['remove_stopwords']\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Also apply the best combination to the test set later after evaluation\n",
    "print(\"Processing complete\")\n",
    "\n",
    "# Display the first few rows with all the combinations\n",
    "df_train_cleaned.iloc[:10, :10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stratify to maintain the distribution of classes in the train, validation, and test sets\n",
    "\n",
    "train_df, val_test_df = train_test_split(df_train_cleaned, test_size=0.3, stratify=df_train_cleaned['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae53e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    6178\n",
      "1    1923\n",
      "0    1442\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    1923\n",
      "0    1442\n",
      "2    1442\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df_train_cleaned.loc[:, df_train_cleaned.columns != 'label']\n",
    "Y = df_train_cleaned['label']\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy=\"majority\")\n",
    "X_resampled, Y_resampled = rus.fit_resample(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e20eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64beec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    4325\n",
       "1    1346\n",
       "0    1009\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b5fbe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6474550898203593"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4325/(4325+1346+1009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66806493",
   "metadata": {},
   "source": [
    "## 1. sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7250ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugof\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "models = dict(\n",
    "    mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2'),\n",
    "    distilroberta = SentenceTransformer('sentence-transformers/all-distilroberta-v1'),\n",
    "    model_3 = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2'),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac32e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de0764e3850444fb3b494ae90fd0ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03c618bed8f4cbdb0c2e545a8fb0175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d4ef6d0234fb89f8e5c1a9a9df0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e5b32a930a4a96bae2290274dad654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc420b6d7949de837937dde6e3fa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6811719525499dbed504d6714d386f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6f1b6151164903906c989618010895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baad05e6ae4d49a5a7be3619ebe1c9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with mpnet for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d937695d16f44b880c75f35d2aa59cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2256bc92d0804b65961cabd7ea79b0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6fe35bf2e04a3b8fb0f329f73db0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1d405848b248e9bc09555f0c1ab39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d5b64e621642b1855feea8c10f3a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a797f3394cd40fa8de32e30bee19a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123768b9ee14b179b11158461fa03a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634b553475354a768d1d8fa853b26c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb555d2887542e98c816b4b811a7e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with distilroberta for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1c6a385d8a40fead443477c2f945db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d392f9c951494653ac110a1161d572f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa20c06fb269455f9255fe50c62230b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf92783d55f247b5832114d0f5e42d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b6c93d3a004071b72a87bf7a899c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e700a713820942dca1225da5372d4b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a9422c11d148c59227e6b6455c82d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8925329591040aaa56f2acb189de1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0f712cdfb440f0a702081ef3d223c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with model_3 for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcc66845248423ba9c8e2b1d0eecd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db307c3aa9949f68376e966da03f950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c5d98b9f8f4e69ab7cfa67e816ed1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30e6a2c401d408eb28b6ae39fa26844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f13975393423c86536546f16f4ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7dc35bf8374b4fa9687303af09b4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8362337965f04a0f83e74aa85a1d6ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473410b5824d4f1c97424f5191c4e3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a630ac4bbf3044b1b147293ee39432d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with mpnet for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e49c9924744b529aff6d706398c6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3f4beb18374829a6141f25682982e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aca15a577146c0a3732cc812a7eac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06beeb187fd2401faa839ff7f5b03c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6835781f2ad5494091c0a0e2890d3c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5075a65e84f4694b983b59275394789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a66824fe1ba42e28b5d18bfa2a59c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9aad8dce53442390c32c5bfc3999dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe13df2094b42309f84fd333b3d6950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with distilroberta for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1219f0b13a4636885bece4326c972e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac64dbf5eff84393923d736b1aaf5ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1871cc346830422f9c6607110de5096a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f57139aca84418eae4116141ca93d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dde35e1380147a7a25d2e55c7742fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb665ecebf64bdaaccb0b12cd8ea41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a470d8f71dc44913a18e2a2246bc6cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a961f79746e9441c92b4bc64c5fa4585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a586a0e11df6495c954f196a350f13da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with model_3 for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d41eb995033437f9c51eff42ccdefb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8a80935861462082de7e5b421c10f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da2c904fd734e6db5c58a995e09c768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1bc5025c5e4d4a8407c3801b1379cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851908458b8e47e5813dec74346c5961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9864ec59ae64189bfe93adcd5473611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bfd0b270b742ee87e55a280bc0a891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cead5592e454d2188ef517a747d1c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e2782cc28a406c986c5edf18f8c8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with mpnet for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae11312f43e74b94904dc89031185212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec640219af549bb8da5eede39b66a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d151da1a3ca43edb03bb6cbac17f26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d023ee0e07e4e63b44deb3a8582ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98636fdc0c54830854b6deb43458440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866bc6dad4ef43b1a4a44b03f061a835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513d6c27965e4992a880439b4ac56a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e141b4ba044cd19b383f601752d840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19187a7f9644711a712dc4027f65a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with distilroberta for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13feff328aec44afa85375af7dcb1c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65989d7492e3436b95eb19a4e0d55065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_with_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270c746fe97a4a4fa39250517d10970d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_with_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7d52e3f7fe45e28653b549dd9fbae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd69d6d4dac54c88af83b9378f5cb8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_no_stem_no_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a20287d6bd430d8edf8c6e1184ea7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_no_stem_no_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9746dea58c40b4ac3e4364eb33d336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_no_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3fb8b2d914662a3c73f98a0fcff97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_with_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c1742643b3433abcbdf792889112e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_lemma_stem_no_stopwords with model_3 for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742ff100920841f7a55a59f1e72abf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = {\n",
    "    'train': train_df,\n",
    "    'val': val_df,\n",
    "    'test': test_df\n",
    "}\n",
    "\n",
    "embedding_results = {\n",
    "    'train': {},\n",
    "    'val': {},\n",
    "    'test': {}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for split_name, df in datasets.items():\n",
    "    values = df.loc[:, df.columns != 'label']\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        for col in values.columns:\n",
    "            print(f\"Encoding {col} with {model_name} for {split_name} set...\")\n",
    "            text_data = df[col].astype(str).tolist()\n",
    "            embeddings = model.encode(text_data, batch_size=64, show_progress_bar=True)\n",
    "            \n",
    "            embedding_results[split_name][col] = embeddings\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('embedding_results_all_splits.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for embedding: text_mpnet\n",
      "Training model for embedding: text_no_lemma_no_stem_with_stopwords_mpnet\n",
      "Training model for embedding: text_lemma_no_stem_with_stopwords_mpnet\n",
      "Training model for embedding: text_no_lemma_stem_with_stopwords_mpnet\n",
      "Training model for embedding: text_no_lemma_no_stem_no_stopwords_mpnet\n",
      "Training model for embedding: text_lemma_no_stem_no_stopwords_mpnet\n",
      "Training model for embedding: text_no_lemma_stem_no_stopwords_mpnet\n",
      "Training model for embedding: text_lemma_stem_with_stopwords_mpnet\n",
      "Training model for embedding: text_lemma_stem_no_stopwords_mpnet\n",
      "Training model for embedding: text_distilroberta\n",
      "Training model for embedding: text_no_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training model for embedding: text_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training model for embedding: text_no_lemma_stem_with_stopwords_distilroberta\n",
      "Training model for embedding: text_no_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training model for embedding: text_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training model for embedding: text_no_lemma_stem_no_stopwords_distilroberta\n",
      "Training model for embedding: text_lemma_stem_with_stopwords_distilroberta\n",
      "Training model for embedding: text_lemma_stem_no_stopwords_distilroberta\n",
      "Training model for embedding: text_model_3\n",
      "Training model for embedding: text_no_lemma_no_stem_with_stopwords_model_3\n",
      "Training model for embedding: text_lemma_no_stem_with_stopwords_model_3\n",
      "Training model for embedding: text_no_lemma_stem_with_stopwords_model_3\n",
      "Training model for embedding: text_no_lemma_no_stem_no_stopwords_model_3\n",
      "Training model for embedding: text_lemma_no_stem_no_stopwords_model_3\n",
      "Training model for embedding: text_no_lemma_stem_no_stopwords_model_3\n",
      "Training model for embedding: text_lemma_stem_with_stopwords_model_3\n",
      "Training model for embedding: text_lemma_stem_no_stopwords_model_3\n",
      "✅ All models trained. Metrics saved to 'embedding_logreg_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Make sure LabelEncoder is ready\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "\n",
    "# Store results here\n",
    "results = []\n",
    "\n",
    "for key in embedding_results['train'].keys():\n",
    "    print(f\"Training model for embedding: {key}\")\n",
    "\n",
    "    # Get embeddings for train/val/test\n",
    "    X_train = np.array(embedding_results['train'][key])\n",
    "    X_val = np.array(embedding_results['val'][key])\n",
    "    y_train = le.transform(train_df['label'])\n",
    "    y_val = le.transform(val_df['label'])\n",
    "\n",
    "    # Train logistic regression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "\n",
    "    # Evaluate\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_val, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    col_name, model_name = key.rsplit('_', 1)\n",
    "    results.append({\n",
    "        'column': col_name,\n",
    "        'embedding_model': model_name,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(results)\n",
    "\n",
    "# Optional: sort by F1-score\n",
    "metrics_df = metrics_df.sort_values(by='f1_score', ascending=False)\n",
    "\n",
    "# Save results to CSV\n",
    "metrics_df.to_csv('embedding_logreg_results.csv', index=False)\n",
    "\n",
    "print(\"✅ All models trained. Metrics saved to 'embedding_logreg_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a7e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM on: text_mpnet\n",
      "Training BiLSTM on: text_no_lemma_no_stem_with_stopwords_mpnet\n",
      "Training BiLSTM on: text_lemma_no_stem_with_stopwords_mpnet\n",
      "Training BiLSTM on: text_no_lemma_stem_with_stopwords_mpnet\n",
      "Training BiLSTM on: text_no_lemma_no_stem_no_stopwords_mpnet\n",
      "Training BiLSTM on: text_lemma_no_stem_no_stopwords_mpnet\n",
      "Training BiLSTM on: text_no_lemma_stem_no_stopwords_mpnet\n",
      "Training BiLSTM on: text_lemma_stem_with_stopwords_mpnet\n",
      "Training BiLSTM on: text_lemma_stem_no_stopwords_mpnet\n",
      "Training BiLSTM on: text_distilroberta\n",
      "Training BiLSTM on: text_no_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM on: text_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM on: text_no_lemma_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM on: text_no_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM on: text_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM on: text_no_lemma_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM on: text_lemma_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM on: text_lemma_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM on: text_model_3\n",
      "Training BiLSTM on: text_no_lemma_no_stem_with_stopwords_model_3\n",
      "Training BiLSTM on: text_lemma_no_stem_with_stopwords_model_3\n",
      "Training BiLSTM on: text_no_lemma_stem_with_stopwords_model_3\n",
      "Training BiLSTM on: text_no_lemma_no_stem_no_stopwords_model_3\n",
      "Training BiLSTM on: text_lemma_no_stem_no_stopwords_model_3\n",
      "Training BiLSTM on: text_no_lemma_stem_no_stopwords_model_3\n",
      "Training BiLSTM on: text_lemma_stem_with_stopwords_model_3\n",
      "Training BiLSTM on: text_lemma_stem_no_stopwords_model_3\n",
      "✅ All BiLSTM models trained. Results saved to 'embedding_bilstm_results.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# BiLSTM model class\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len=1, input_dim]\n",
    "        output, _ = self.lstm(x)\n",
    "        out = self.fc(output[:, -1, :])  # Take output of the last time step\n",
    "        return out\n",
    "\n",
    "# Store metrics\n",
    "results_bilstm = []\n",
    "\n",
    "# Loop over each embedding variant\n",
    "for key in embedding_results['train'].keys():\n",
    "    print(f\"Training BiLSTM on: {key}\")\n",
    "\n",
    "    # Get embeddings\n",
    "    X_train = torch.tensor(embedding_results['train'][key], dtype=torch.float32)\n",
    "    X_val = torch.tensor(embedding_results['val'][key], dtype=torch.float32)\n",
    "    y_train = torch.tensor(le.transform(train_df['label']), dtype=torch.long)\n",
    "    y_val = torch.tensor(le.transform(val_df['label']), dtype=torch.long)\n",
    "\n",
    "    # Add fake sequence dimension: [batch_size, seq_len=1, input_dim]\n",
    "    X_train = X_train.unsqueeze(1)\n",
    "    X_val = X_val.unsqueeze(1)\n",
    "\n",
    "    # Datasets and loaders\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model setup\n",
    "    input_dim = X_train.shape[-1]\n",
    "    model = BiLSTMClassifier(input_dim, HIDDEN_DIM, num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            outputs = model(xb)\n",
    "            predicted = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(predicted)\n",
    "            all_true.extend(yb.numpy())\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    col_name, model_name = key.rsplit('_', 1)\n",
    "    results_bilstm.append({\n",
    "        'column': col_name,\n",
    "        'embedding_model': model_name,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Final metrics DataFrame\n",
    "metrics_df_bilstm = pd.DataFrame(results_bilstm)\n",
    "metrics_df_bilstm = metrics_df_bilstm.sort_values(by='f1_score', ascending=False)\n",
    "metrics_df_bilstm.to_csv('embedding_bilstm_results.csv', index=False)\n",
    "\n",
    "print(\"✅ All BiLSTM models trained. Results saved to 'embedding_bilstm_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiGRU on: text_mpnet\n",
      "Training BiGRU on: text_no_lemma_no_stem_with_stopwords_mpnet\n",
      "Training BiGRU on: text_lemma_no_stem_with_stopwords_mpnet\n",
      "Training BiGRU on: text_no_lemma_stem_with_stopwords_mpnet\n",
      "Training BiGRU on: text_no_lemma_no_stem_no_stopwords_mpnet\n",
      "Training BiGRU on: text_lemma_no_stem_no_stopwords_mpnet\n",
      "Training BiGRU on: text_no_lemma_stem_no_stopwords_mpnet\n",
      "Training BiGRU on: text_lemma_stem_with_stopwords_mpnet\n",
      "Training BiGRU on: text_lemma_stem_no_stopwords_mpnet\n",
      "Training BiGRU on: text_distilroberta\n",
      "Training BiGRU on: text_no_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training BiGRU on: text_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training BiGRU on: text_no_lemma_stem_with_stopwords_distilroberta\n",
      "Training BiGRU on: text_no_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training BiGRU on: text_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training BiGRU on: text_no_lemma_stem_no_stopwords_distilroberta\n",
      "Training BiGRU on: text_lemma_stem_with_stopwords_distilroberta\n",
      "Training BiGRU on: text_lemma_stem_no_stopwords_distilroberta\n",
      "Training BiGRU on: text_model_3\n",
      "Training BiGRU on: text_no_lemma_no_stem_with_stopwords_model_3\n",
      "Training BiGRU on: text_lemma_no_stem_with_stopwords_model_3\n",
      "Training BiGRU on: text_no_lemma_stem_with_stopwords_model_3\n",
      "Training BiGRU on: text_no_lemma_no_stem_no_stopwords_model_3\n",
      "Training BiGRU on: text_lemma_no_stem_no_stopwords_model_3\n",
      "Training BiGRU on: text_no_lemma_stem_no_stopwords_model_3\n",
      "Training BiGRU on: text_lemma_stem_with_stopwords_model_3\n",
      "Training BiGRU on: text_lemma_stem_no_stopwords_model_3\n",
      "✅ All BiGRU models trained. Results saved to 'embedding_bigru_results.csv'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# BiGRU model\n",
    "class BiGRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiGRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len=1, input_dim]\n",
    "        output, _ = self.gru(x)\n",
    "        out = self.fc(output[:, -1, :])  # Last time step\n",
    "        return out\n",
    "\n",
    "# Store results\n",
    "results_bigru = []\n",
    "\n",
    "# Loop over all embedding keys\n",
    "for key in embedding_results['train'].keys():\n",
    "    print(f\"Training BiGRU on: {key}\")\n",
    "\n",
    "    # Prepare data\n",
    "    X_train = torch.tensor(embedding_results['train'][key], dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(embedding_results['val'][key], dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(le.transform(train_df['label']), dtype=torch.long)\n",
    "    y_val = torch.tensor(le.transform(val_df['label']), dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model init\n",
    "    input_dim = X_train.shape[-1]\n",
    "    model = BiGRUClassifier(input_dim, HIDDEN_DIM, num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xb)\n",
    "            loss = criterion(output, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "            all_preds.extend(pred_labels)\n",
    "            all_true.extend(yb.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    col_name, model_name = key.rsplit('_', 1)\n",
    "    results_bigru.append({\n",
    "        'column': col_name,\n",
    "        'embedding_model': model_name,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "metrics_df_bigru = pd.DataFrame(results_bigru)\n",
    "metrics_df_bigru = metrics_df_bigru.sort_values(by='f1_score', ascending=False)\n",
    "metrics_df_bigru.to_csv('embedding_bigru_results.csv', index=False)\n",
    "\n",
    "print(\"✅ All BiGRU models trained. Results saved to 'embedding_bigru_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1955adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM + Attention on: text_mpnet\n",
      "Training BiLSTM + Attention on: text_no_lemma_no_stem_with_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_lemma_no_stem_with_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_no_lemma_stem_with_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_no_lemma_no_stem_no_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_lemma_no_stem_no_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_no_lemma_stem_no_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_lemma_stem_with_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_lemma_stem_no_stopwords_mpnet\n",
      "Training BiLSTM + Attention on: text_distilroberta\n",
      "Training BiLSTM + Attention on: text_no_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_no_lemma_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_no_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_no_lemma_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_lemma_stem_with_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_lemma_stem_no_stopwords_distilroberta\n",
      "Training BiLSTM + Attention on: text_model_3\n",
      "Training BiLSTM + Attention on: text_no_lemma_no_stem_with_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_lemma_no_stem_with_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_no_lemma_stem_with_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_no_lemma_no_stem_no_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_lemma_no_stem_no_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_no_lemma_stem_no_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_lemma_stem_with_stopwords_model_3\n",
      "Training BiLSTM + Attention on: text_lemma_stem_no_stopwords_model_3\n",
      "✅ All BiLSTM + Attention models trained. Saved to 'embedding_bilstm_attention_results.csv'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# Attention layer\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        # lstm_out: [batch_size, seq_len, hidden_dim*2]\n",
    "        attn_weights = torch.softmax(self.attn(lstm_out), dim=1)  # [batch_size, seq_len, 1]\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)       # [batch_size, hidden_dim*2]\n",
    "        return context\n",
    "\n",
    "# BiLSTM + Attention classifier\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_output = self.attention(lstm_out)\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "# Store results\n",
    "results_bilstm_attn = []\n",
    "\n",
    "# Loop over embeddings\n",
    "for key in embedding_results['train'].keys():\n",
    "    print(f\"Training BiLSTM + Attention on: {key}\")\n",
    "\n",
    "    # Get data\n",
    "    X_train = torch.tensor(embedding_results['train'][key], dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(embedding_results['val'][key], dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(le.transform(train_df['label']), dtype=torch.long)\n",
    "    y_val = torch.tensor(le.transform(val_df['label']), dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model setup\n",
    "    input_dim = X_train.shape[-1]\n",
    "    model = BiLSTMWithAttention(input_dim, HIDDEN_DIM, num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            output = model(xb)\n",
    "            pred_labels = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            all_preds.extend(pred_labels)\n",
    "            all_true.extend(yb.numpy())\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    col_name, model_name = key.rsplit('_', 1)\n",
    "    results_bilstm_attn.append({\n",
    "        'column': col_name,\n",
    "        'embedding_model': model_name,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "metrics_df_bilstm_attn = pd.DataFrame(results_bilstm_attn)\n",
    "metrics_df_bilstm_attn = metrics_df_bilstm_attn.sort_values(by='f1_score', ascending=False)\n",
    "metrics_df_bilstm_attn.to_csv('embedding_bilstm_attention_results.csv', index=False)\n",
    "\n",
    "print(\"✅ All BiLSTM + Attention models trained. Saved to 'embedding_bilstm_attention_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb8807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN on: text_mpnet\n",
      "Training CNN on: text_no_lemma_no_stem_with_stopwords_mpnet\n",
      "Training CNN on: text_lemma_no_stem_with_stopwords_mpnet\n",
      "Training CNN on: text_no_lemma_stem_with_stopwords_mpnet\n",
      "Training CNN on: text_no_lemma_no_stem_no_stopwords_mpnet\n",
      "Training CNN on: text_lemma_no_stem_no_stopwords_mpnet\n",
      "Training CNN on: text_no_lemma_stem_no_stopwords_mpnet\n",
      "Training CNN on: text_lemma_stem_with_stopwords_mpnet\n",
      "Training CNN on: text_lemma_stem_no_stopwords_mpnet\n",
      "Training CNN on: text_distilroberta\n",
      "Training CNN on: text_no_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training CNN on: text_lemma_no_stem_with_stopwords_distilroberta\n",
      "Training CNN on: text_no_lemma_stem_with_stopwords_distilroberta\n",
      "Training CNN on: text_no_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training CNN on: text_lemma_no_stem_no_stopwords_distilroberta\n",
      "Training CNN on: text_no_lemma_stem_no_stopwords_distilroberta\n",
      "Training CNN on: text_lemma_stem_with_stopwords_distilroberta\n",
      "Training CNN on: text_lemma_stem_no_stopwords_distilroberta\n",
      "Training CNN on: text_model_3\n",
      "Training CNN on: text_no_lemma_no_stem_with_stopwords_model_3\n",
      "Training CNN on: text_lemma_no_stem_with_stopwords_model_3\n",
      "Training CNN on: text_no_lemma_stem_with_stopwords_model_3\n",
      "Training CNN on: text_no_lemma_no_stem_no_stopwords_model_3\n",
      "Training CNN on: text_lemma_no_stem_no_stopwords_model_3\n",
      "Training CNN on: text_no_lemma_stem_no_stopwords_model_3\n",
      "Training CNN on: text_lemma_stem_with_stopwords_model_3\n",
      "Training CNN on: text_lemma_stem_no_stopwords_model_3\n",
      "✅ All CNN models trained. Results saved to 'embedding_cnn_results.csv'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label encoder (reuse)\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "KERNEL_SIZE = 3\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "# CNN model for 1D embedding vectors\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, kernel_size, output_dim):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, num_filters, kernel_size, padding=kernel_size//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(num_filters, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # add channel dim here\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Store metrics\n",
    "results_cnn = []\n",
    "\n",
    "for key in embedding_results['train'].keys():\n",
    "    print(f\"Training CNN on: {key}\")\n",
    "\n",
    "    # Get embeddings & labels\n",
    "    X_train = torch.tensor(embedding_results['train'][key], dtype=torch.float32)\n",
    "    X_val = torch.tensor(embedding_results['val'][key], dtype=torch.float32)\n",
    "    y_train = torch.tensor(le.transform(train_df['label']), dtype=torch.long)\n",
    "    y_val = torch.tensor(le.transform(val_df['label']), dtype=torch.long)\n",
    "\n",
    "    # Datasets and loaders\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model init\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = CNNClassifier(input_dim, NUM_FILTERS, KERNEL_SIZE, num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "            all_preds.extend(pred_labels)\n",
    "            all_true.extend(yb.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    col_name, model_name = key.rsplit('_', 1)\n",
    "    results_cnn.append({\n",
    "        'column': col_name,\n",
    "        'embedding_model': model_name,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Save metrics\n",
    "metrics_df_cnn = pd.DataFrame(results_cnn).sort_values(by='f1_score', ascending=False)\n",
    "metrics_df_cnn.to_csv('embedding_cnn_results.csv', index=False)\n",
    "\n",
    "print(\"✅ All CNN models trained. Results saved to 'embedding_cnn_results.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
